# -*- coding: utf-8 -*-
"""Reto_Contaminacion_del_Aire.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFQehXpFRyTl79vjIiJ3spiRIVblAPp1

<font face = "Times New Roman" size = "3" color = "black">
<center>
<h3></h3>
<img src = "https://proleon.com.mx/wp-content/uploads/2022/10/01_TM_ESPECIAL_INNOVACION_EDUCATIVA_UNIVERSIDADES_TEC_DE_MONTERREY.png" width = "250">
<h3><b>Campus Estado de México</h3>
<h3>Aplicación de Métodos Multivariados en Ciencia de Datos</b><br>MA2003B.101 2024-13<br><br><b>Evidencia 2. Contaminación del Aire en el Área Metropolitana de Monterrey (AMM)<br><br>Omar Rodríguez Montiel A01750836<br><b>Profesores</b><br>Saúl Juárez Ordóñez<br>Pablo Mendoza Iturralde<br>Faustino Yescas Martínez</h3>
<img src = "https://images.reporteindigo.com/wp-content/uploads/2023/01/mala-contaminacion-del-aire-monterrey.jpg" width = "500">
<h6>Crédito de la imagen: <a href = "https://www.reporteindigo.com/reporte/calidad-del-aire-en-zona-metropolitana-entre-regular-y-mala/">Calidad del Aire en Zona Metropolitana de Monterrey</a></h6>
</img>
</center>
</font>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Resumen</b></h2>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h4>Este estudio investigó la contaminación del aire en el Área Metropolitana de Monterrey mediante el análisis de datos de PM2.5 y factores meteorológicos utilizando técnicas estadísticas y de aprendizaje automático. Se aplicó regresión lineal múltiple y PCA para predecir los niveles de PM2.5, encontrando una correlación moderada entre PM2.5 y las dos primeras componentes principales (PC1 y PC2). La regresión con PCA mostró un R^2 ajustado de 0.2635 para el entrenamiento y -0.1338 para la prueba, lo que indica sobreajuste en el modelo. El análisis de <i>K-Means</i> con K = 2 presentó un coeficiente de silueta de 0.29, mientras que K = 3 mostró una mejora con un coeficiente de silueta de 0.3687. Estos resultados sugieren que el modelo con K = 3 proporciona una mejor separación entre grupos, aunque la técnica de PCA puede haber implicado pérdida de información.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b><i>Abstract<i></b></h2>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h4><i>This project investigated air pollution in the Monterrey Metropolitan Area by analyzing PM2.5 data and meteorological factors using statistical and machine learning techniques. Multiple linear regression and PCA were applied to predict PM2.5 levels, finding a moderate correlation between PM2.5 and the first two principal components (PC1 and PC2). The PCA regression showed an adjusted R^2 of 0.2635 for training and -0.1338 for testing, indicating overfitting in the model. K-means analysis with K=2 presented a silhouette coefficient of 0.29, while K=3 showed an improvement with a silhouette coefficient of 0.3687. These results suggest that the model with K=3 provides better separation between groups, although the PCA technique may have implied loss of information.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Palabras clave</b> (<i>keywords</i>)</h2>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h4>Contaminación del aire, PM2.5, Análisis de Componentes Principales, PCA, <i>K-Means</i>, Regresión Lineal Múltiple, Área Metropolitana de Monterrey, Análisis de Datos.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Introducción</b></h2>
<h4>La contaminación del aire es un problema crítico que afecta la salud pública y el medio ambiente en áreas urbanas. En el Área Metropolitana de Monterrey (AMM), la calidad del aire ha sido motivo de creciente preocupación debido a los altos niveles de contaminantes atmosféricos como PM2.5, que pueden tener efectos adversos en la salud respiratoria y cardiovascular. Este proyecto utiliza técnicas avanzadas de análisis de datos, como el Análisis de Componentes Principales (PCA) y el algoritmo <i>K-Means</i>, para examinar la relación entre los niveles de PM2.5 y varios factores meteorológicos, con el objetivo de identificar patrones y mejorar la comprensión de los factores que contribuyen a la contaminación del aire en la región.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Descripción de la problemática</b></h2>
<h4>La contaminación del aire en el Área Metropolitana de Monterrey (AMM) se ha convertido en un problema serio que impacta tanto la salud pública como el medio ambiente. La elevada presencia de contaminantes como monóxido de carbono (CO), dióxido de nitrógeno (NO<sub>2</sub>), dióxido de azufre (SO<sub>2</sub>), y partículas PM2.5, causada por actividades humanas tales como la industrialización, el transporte y la quema de combustibles fósiles, ha llevado a una preocupante reducción en la calidad del aire en la región (Zúñiga Martínez, 2023). De acuerdo con la Agencia de Protección Ambiental de Estados Unidos, las partículas PM2.5 y PM10 son tipos de material particulado en el aire. Las PM10 son partículas inhalables que tienen diámetros de hasta 10 micrómetros, mientras que las PM2.5 son partículas finas más pequeñas, de hasta 2.5 micrómetros de diámetro, lo que las hace 30 veces más pequeñas que un cabello humano promedio. Estas partículas pueden provenir de fuentes directas como obras de construcción o incendios, o formarse en la atmósfera a partir de reacciones químicas de contaminantes emitidos por vehículos e industrias.<br><br>Este deterioro se manifiesta no solo en la aparición de una niebla gris y olores desagradables, sino también en un incremento de enfermedades respiratorias y cardiovasculares, así como en otros efectos adversos sobre la salud de los residentes y la biodiversidad local (Fundación Aquae, 2023).<br><br>Para enfrentar este desafío, es crucial desarrollar herramientas analíticas avanzadas que permitan comprender las dinámicas de la contaminación y su relación con factores meteorológicos. Un enfoque que utilice modelos de regresión, análisis de conglomerados y análisis de componentes principales podría ofrecer soluciones clave, facilitando la identificación de patrones y la predicción de niveles de contaminantes como las partículas PM2.5. Además, el análisis comparativo de la contaminación durante períodos de baja circulación vehicular, como los experimentados durante la pandemia de COVID-19, podría abrir la puerta a la implementación de políticas ambientales más efectivas y mejorar la calidad del aire en el AMM (Seguí, P.).</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Preguntas de investigación</b></h2>
<h3><ol type="1"><li>¿Cuáles son las variables meteorológicas que tienen mayor impacto en la concentración de partículas PM2.5 en el Área Metropolitana de Monterrey?</li><li>¿Cómo se agrupan los distintos contaminantes del aire en el AMM y qué patrones emergen dentro de estos grupos?</li><li>¿Es posible predecir con precisión los niveles de PM2.5 utilizando otros contaminantes y condiciones meteorológicas como referencia?</li><li>¿Cómo influye la variación estacional en la calidad del aire en el AMM y cuál es su relación con los diferentes contaminantes?</li></ol></h3>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Métodología utilizada</b></h2>
<h3><b>Pre-procesamiento</h3></b><h4>En el preprocesamiento de los datos de la serie temporal que contiene las cantidades de contaminantes y partículas PM10 y PM2.5, es fundamental abordar primero la estructuración de las marcas de tiempo. Esto implica asegurarse de que los datos estén correctamente ordenados y que la columna de fecha y hora tenga el tipo de datos adecuado para facilitar el análisis posterior. Además, es crucial identificar y tratar los valores faltantes, tanto de tiempo como de datos, ya que pueden sesgar los resultados si no se manejan adecuadamente. Métodos como la interpolación lineal, el relleno hacia adelante o hacia atrás, y la descomposición estacional y de tendencia pueden ser útiles para imputar estos valores, garantizando así la integridad de la serie temporal (Gupta, 2022; Data Science Wizards, 2023).<br><b>Tratamiento de Datos Faltantes</b><br><ol>
  <li><b>Interpolación</b>: Este método rellena los valores faltantes basándose en una función matemática que asume una transición suave entre los puntos conocidos adyacentes. Puede ser especialmente útil en series de tiempo con tendencias suaves, ya que preserva la continuidad y la tendencia de los datos. Sin embargo, en series con alta variabilidad, puede no capturar las fluctuaciones de manera precisa. Existen distintos métodos de llevar a cabo esta imputación de datos faltantes.</li>
    <ul>
      <li><b>Lineal</b>: Es una técnica comúnmente utilizada para imputar valores faltantes en series temporales. Consiste en trazar una línea recta entre los puntos adyacentes a los valores faltantes y estimar los datos intermedios basándose en esta línea. Este método asume que los cambios entre los puntos son constantes y uniformes, lo que lo convierte en una opción adecuada para datos que muestran una progresión suave y lineal. Es efectiva cuando la serie de tiempo sigue una tendencia clara o una relación lineal entre observaciones. No obstante, si la serie presenta alta variabilidad o comportamientos no lineales, este método puede ser insuficiente, ya que no captura patrones complejos como estacionalidades o fluctuaciones repentinas (Hosseinabadi, 2022).</li>
      <li><b><i>Spline</b></i>: Este método es una técnica avanzada de interpolación que utiliza funciones spline, específicamente splines cúbicos, para ajustar una serie de polinomios en tramos entre cada par de puntos de datos adyacentes. La interpolación spline es particularmente útil para datos que presentan cambios suaves pero no necesariamente lineales, ya que garantiza que la curva resultante sea suave en las uniones entre tramos. Este método es ideal para situaciones donde la continuidad y la suavidad de la curva interpolada son importantes​ (Crawstat, 2020).</li>
      <li><b>Polinómica</b>: La interpolación polinómica consiste en ajustar un único polinomio de grado n-1 a través de todos los puntos de datos conocidos. A medida que el grado del polinomio aumenta, la curva resultante puede volverse más precisa, pero también puede ser propensa a oscilaciones y comportamientos no deseados, especialmente en los extremos de los datos. Este método es adecuado cuando los datos siguen una tendencia que puede ser bien representada por una función polinómica, aunque se debe tener cuidado con los problemas de sobreajuste​ (Crawstat, 2020).</li>
    </ul>
  <li><b>Relleno hacia adelante y hacia atrás (<i>Forward / Backward Fill</i>)</b>: El método de relleno hacia adelante usa el último valor conocido antes de un dato faltante para llenar el vacío, mientras que el método hacia atrás utiliza el siguiente valor disponible. Estos métodos son simples y efectivos en situaciones donde se asume que los valores no cambian drásticamente en cortos periodos. Sin embargo, pueden introducir sesgos si los datos cambian significativamente (Crawstat, 2020; Hosseinabadi, 2022)​.</li>
  <li><b>Descomposición de tendencias y estacionalidad</b>: Este enfoque implica descomponer la serie de tiempo en componentes de tendencia y estacionalidad para imputar los valores faltantes en cada componente por separado. Este método es eficaz en series de tiempo donde los patrones estacionales o de tendencia son fuertes, permitiendo una imputación más precisa que respeta la estructura temporal subyacente (Crawstat, 2020).</li>
  <li><b><i>Exponential Smoothing</i> Aditivo</b>: El <i>Exponential Smoothing</i> Aditivo es un método de suavizado que se emplea principalmente en series temporales con tendencia. Este enfoque calcula un promedio ponderado de los datos pasados, donde los pesos disminuyen exponencialmente con el tiempo. En el contexto de imputación de datos faltantes, este método es útil ya que permite mantener la tendencia en los datos mientras se interpolan los valores faltantes, proporcionando una estimación más precisa y coherente con la naturaleza del conjunto de datos. Este método es particularmente relevante en modelos de regresión donde es crucial preservar las características temporales de los datos para obtener predicciones confiables (Hyndman & Athanasopoulos, 2018; Taylor, 2003).</li>
  <li><b><i>Exponential Smoothing</i> Multiplicativo</b>: Por otro lado, el <i>Exponential Smoothing</i> Multiplicativo se utiliza cuando los datos presentan tanto tendencia como estacionalidad, y las variaciones estacionales multiplican los niveles de la serie temporal. Este método es clave para la imputación de datos faltantes en situaciones donde la estacionalidad juega un papel importante en la variabilidad de los datos. Al aplicar un suavizado exponencial multiplicativo, se pueden rellenar los valores faltantes respetando tanto la tendencia como las variaciones estacionales, lo que es esencial para garantizar que los modelos de regresión mantengan su precisión al predecir valores futuros en presencia de estacionalidad (Hyndman & Athanasopoulos, 2018; Snyder et al., 2017).</li>
</ol><b>Descomposición de la Serie Temporal y Eliminación de Ruido</b><br>Asimismo, la eliminación de ruido en la serie temporal es un paso esencial para mejorar la precisión del análisis. Técnicas como las medias móviles y la Transformada de Fourier pueden ayudar a reducir las fluctuaciones no deseadas en los datos, proporcionando un conjunto de datos más limpio y preparado para el modelado. Al seguir estos pasos de preprocesamiento, se puede asegurar que los datos estén listos para ser utilizados en modelos más complejos, lo que permitirá obtener resultados más precisos y fiables (Aldean, 2023).<br>
<ul>
  <li><b>Media Móvil (<i>Rolling Mean</i> ó <i>Moving Average</i> (MA))</b>: La media móvil es una técnica utilizada para suavizar una serie temporal, donde se calcula el promedio de un conjunto de observaciones previas dentro de una ventana específica. Esta media se computa de forma secuencial para cada ventana a lo largo de la serie, lo cual ayuda a reducir significativamente el ruido presente en los datos (Gupta, 2022).</li>
  <li><b>Transformada de Fourier</b>: La transformada de Fourier ofrece un método para eliminar el ruido al trasladar los datos de una serie temporal al dominio de la frecuencia. En este espacio, se pueden filtrar las frecuencias que contribuyen al ruido, y luego, mediante la transformada de Fourier inversa, se reconstruye la serie temporal, pero ahora con un nivel de ruido reducido (Gupta, 2022).</li>
</ul></h4>
<h3><b>Regresión Lineal Múltiple</b></h3>
<h4>La regresión lineal múltiple es una técnica estadística que permite modelar la relación entre una variable dependiente y varias variables independientes. Este método es fundamental en el análisis de datos porque permite evaluar el impacto conjunto de múltiples factores en una variable de interés, lo que es crucial para hacer predicciones precisas y tomar decisiones informadas en proyectos de análisis de datos. La ecuación general de la regresión lineal múltiple se expresa como:<br>
<center>
<p style="text-align: center;">
    <h4><b><i>y</i> = <i>β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + … + β<sub>n</sub>x<sub>n</sub></i></b></h4>
</p>
</center>
donde <i>𝑦</i> es la variable dependiente, <i>𝑥</i><sub>1</sub>, <i>𝑥</i><sub>2</sub>, ..., <i>𝑥</i><sub><i>n</i></sub> son las variables independientes, 𝛽<sub>0</sub> es la intersección y 𝛽<sub>1</sub>, 𝛽<sub>2</sub>, ..., 𝛽<sub><i>n</i></sub> son los coeficientes asociados a cada variable independiente. La importancia de este modelo radica en su capacidad para capturar la influencia simultánea de múltiples variables, lo que es particularmente útil en análisis económicos, científicos y sociales. (Aggarwal, 2020; <i>Geeks for Geeks</i>, 2024).<br>En el contexto de <i>Python</i>, la biblioteca <i>Scikit-Learn</i> (o simplemente <i>sklearn</i>) facilita la implementación de la regresión lineal múltiple a través de la clase <i>LinearRegression</i>, que se encuentra en el módulo <i>linear_model</i>. Esta herramienta permite ajustar el modelo a los datos, realizar predicciones y analizar la importancia de cada coeficiente en la ecuación, haciendo que la regresión lineal múltiple sea accesible y eficiente para manejar grandes conjuntos de datos. La <i>LinearRegression</i> en <i>sklearn</i> también incluye funciones para evaluar el rendimiento del modelo, lo que la convierte en una opción preferida para analistas de datos que buscan combinar simplicidad con potencia en sus análisis (<i>sklearn</i>, s.f.).</h4>

<h3><b><i>K-Means</i></b></h3>
<h4>
El algoritmo <i>K-Means</i> es una herramienta poderosa para la identificación y análisis de conglomerados en conjuntos de datos grandes y complejos. Su simplicidad y efectividad lo han convertido en un estándar en la minería de datos y el aprendizaje automático, permitiendo a los profesionales identificar patrones y relaciones que no serían evidentes a través de métodos más tradicionales (Sanz, 2020).<br>En estadística un conglomerado (o clúster) es un grupo de objetos que son similares entre sí y diferentes de objetos en otros grupos. El algoritmo <i>K-Means</i> busca identificar estos conglomerados dentro de un conjunto de datos, lo que es fundamental en muchos campos, como la segmentación de mercado, la identificación de patrones en imágenes y la clasificación de documentos (Sanz, 2020; Hartigan & Wong, 1979).<br><i>K-Means</i> es particularmente útil para identificar conglomerados cuando la estructura de los datos no es evidente a simple vista. Al dividir los datos en conglomerados, <i>K-Means</i> facilita la comprensión de las relaciones internas dentro de un conjunto de datos, permitiendo a los investigadores y analistas extraer conocimiento valioso de la información disponible (Hartigan & Wong, 1979).
</h4>

<h3><b>Análisis de Componentes Principales (PCA)</b></h3>
<h4>
El Análisis de Componentes Principales (PCA, por sus siglas en inglés - <i>Principal Component Analysis</i>) es una técnica de reducción de dimensionalidad que transforma un conjunto de variables posiblemente correlacionadas en un conjunto de valores de variables no correlacionadas llamadas componentes principales. Este método es particularmente útil cuando se trabaja con datos de alta dimensionalidad, ya que permite simplificar el modelo sin perder información crítica. PCA identifica las direcciones (componentes) en las que los datos varían más, permitiendo que los datos se proyecten en un espacio de menor dimensión, lo cual es crucial en tareas de clasificación, regresión y visualización de datos complejos (Jolliffe, 2016; Shlens, 2014).<br>En Python, la biblioteca <i>sklearn</i> ofrece la implementación del PCA a través de la clase PCA en el módulo <i>decomposition</i>. Esta herramienta permite extraer las componentes principales de un conjunto de datos, facilitando así la reducción de dimensionalidad y la visualización de datos en 2D o 3D. Además, <i>sklearn</i> proporciona métodos para determinar la cantidad de varianza explicada por cada componente, ayudando a seleccionar el número adecuado de componentes a conservar. Esto es esencial para optimizar el rendimiento de los modelos y evitar el sobreajuste (Pedregosa et al., 2011; Raschka, 2015).
</h4>
<h3>En el contexto de un PCA, es esencial calcular el factor de inflación de la varianza (VIF, por sus siglas en inglés - <i>Variance Inflation Factor</i>) para identificar y reducir la multicolinealidad, que puede distorsionar los resultados y la interpretación del análisis.<br>
<b>Factor de inflación de la varianza (VIF)</b></h3>
<h4>El Factor de Inflación de la Varianza (VIF) es una medida que evalúa la multicolinealidad entre las variables independientes en un modelo de regresión. Si los valores de VIF son elevados, significa que existe una fuerte correlación entre las variables, lo que podría interferir con la efectividad del PCA al intentar reducir la dimensionalidad del conjunto de datos. Por lo tanto, reducir la multicolinealidad mediante la revisión de los VIF es un paso crucial para garantizar que el PCA funcione correctamente, proporcionando componentes principales que capturen mejor la varianza en los datos originales (Chatterjee & Hadi, 2015). De acuerdo con Hernández, Usuga y Mazo (2024), los siguientes son los valores a considerar en la obtención del VIF:<br>
<ul>
<li>Si VIF ≤ 5, entonces no hay problemas de multicolinealidad.</li>
<li>Si 5 < VIF ≤ 10, entonces hay problemas de multicolinealidad moderada.</li>
<li>Si VIF > 10, entonces hay problemas de multicolinealidad graves.</li>
</ul>
</h4>
"""

'''
Librerías para el tratamiento y visualización básicos de los datos, así como para evitar mensajes innecesarios.
'''
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings("ignore")

'''
Librerías para la imputación de datos faltantes, segmentación, estandarización y análisis de componentes principales
'''
from statsmodels.tsa.holtwinters import ExponentialSmoothing as ES
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

'''
Para evitar multicolinealidad con el PCA
'''
from statsmodels.stats.outliers_influence import variance_inflation_factor
from patsy import dmatrices

'''
Librerías y métodos para la regresión lineal y evaluación
'''
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import pearsonr
import statsmodels.api as sm

'''
Librerías para obtener estadísticos
'''
import scipy.stats as ss
import statsmodels.api as sm

'''
Se importan los componentes necesarios para el análisis de conglomerados, incluyendo su evaluación para la diferenciación
'''
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Lectura del archivo
df = pd.read_csv('Obispado_allmonths (3).csv')
df.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)
df

# Información del dataset
df.info()

# Estadística descriptiva del dataset
df.describe().T

# Observación de los datos faltantes
print(f"Cantidad de datos faltantes: {df.isnull().sum().sum()}")
print(f"Cantidad de datos faltantes por columna:\n{df.isnull().sum()}")

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3>
El dataset descrito consta de 120 entradas con 12 columnas, de las cuales una es de tipo <i>object (Date)</i> y las otras 11 son de tipo <i>float64</i>. Se observa que la columna <i>PM2.5</i> tiene 9 valores faltantes. Se experimentarán diferentes métodos de imputación de datos faltantes, como interpolado lineal, <i>spline</i> de orden 2 y 3, <i>polynomial</i> de orden 2 y 3, <i>forward</i> y <i>backward filling</i>, media móvil, y suavizamiento exponencial aditivo y multiplicativo, así como una regresión preliminar (PR) para datos no faltantes, para posteriormente emplear estos datos en regresión lineal, PCA y K-Means. Se observarán sus dimensiones, analizarán sus tendencias en los periodos faltantes en comparación con el histórico de datos y, en función de sus errores, se seleccionará el mejor método de imputación de datos. Para que tener la certeza de que el modelo da resultados confiables, se realizará una prueba global del mismo y, con ello, poder rechazar o aceptar con evidencia los datos imputados.
</h3>
"""

# Regresión preliminar para sustituir datos faltantes
df_train = df[df['PM2.5'].isna()]
df_test = df[df['PM2.5']>0]
X_train = df.drop(['Date', 'PM2.5'], axis=1)
Y = df['PM2.5']

df_test_X = df_test.drop(['Date', 'PM2.5'], axis=1)
df_test_Y = df_test['PM2.5']

model = LinearRegression()

# Se ajusta el modelo con datos de entrenamiento
model.fit(df_test_X, df_test_Y)

# Se realizan predicciones con los datos de prueba
y_pred = model.predict(df_test_X)

# Calculamos el MSE con datos no faltantes
mse_test = mean_squared_error(df_test_Y, y_pred)

# Predecimos valores faltantes con la regresión
df_train_X = df_train.drop(['Date', 'PM2.5'], axis=1)
predicted_pm25 = model.predict(df_train_X)

# Reemplazamos los valores faltantes con los de la nueva predicción
df_train['PM2.5'] = predicted_pm25

# Concatenamos los datos de entrenamiento y prueba
df_combined = pd.concat([df_train, df_test], axis=0)

# Ordenamos los datos por fecha, como
df_combined.sort_values(by='Date', inplace=True)

# Colocamos los datos de la predicción en una nueva columna: Regresión Preliminar (PR)
df['PM2.5_PR'] = df_combined['PM2.5']

# Interpolación, FFill y BFill
for method in ['linear', 'spline', 'polynomial', 'ffill', 'bfill']:
  if method == 'linear':
    df['PM2.5_IL'] = df['PM2.5'].interpolate(method=method) # Método IL
  else:
    # Interpolados spline y polinomial de orden 2 y 3
    if method == 'spline':
      for order in range(2, 4):
        df['PM2.5_SP'+str(order)] = df['PM2.5'].interpolate(method=method, order=order)
    if method == 'polynomial':
      for order in range(2, 4):
        df['PM2.5_PL'+str(order)] = df['PM2.5'].interpolate(method=method, order=order)
    # Métodos ffill y bfill
    if method == 'ffill':
      df['PM2.5_FF'] = df['PM2.5'].ffill()
    if method == 'bfill':
      df['PM2.5_BF'] = df['PM2.5'].bfill()

# Imputación con Media Móvil
df['PM2.5 Media Móvil'] = df['PM2.5'].fillna(df['PM2.5'].rolling(window = 12, min_periods = 1).mean())

# Suavizamiento Exponencial Aditivo
model = ES(df['PM2.5'].dropna(), trend='add', seasonal='add', seasonal_periods=12)
fit = model.fit()
df['PM2.5_ES+'] = df['PM2.5']
df.loc[df['PM2.5_ES+'].isna(), 'PM2.5_ES+'] = fit.predict(start=0, end=len(df)-1)[df['PM2.5_ES+'].isna()]

# Suavizamiento Exponencial Multiplicativo
model = ES(df['PM2.5'].dropna(), trend='mul', seasonal='mul', seasonal_periods=12)
fit = model.fit()
df['PM2.5_ES*'] = df['PM2.5']
df.loc[df['PM2.5_ES*'].isna(), 'PM2.5_ES*'] = fit.predict(start=0, end=len(df)-1)[df['PM2.5_ES*'].isna()]

# Observación
df

# Estadísticos descriptivos de nuevas columnas
df[['PM2.5'] + [col for col in df.columns if 'PM2.5' in col and not df[col].isnull().any()]].describe().T

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b><i>Insights</i></b></h3>
<h4>
Los nuevos datos candidatos para la imputación cuentan con las métricas anteriores. Se aprecia, en la tabla de estadísticos descriptivos, que PM2.5 con Spline de orden 2 y 3 cuenta con dimensiones, incluyendo desviación estándar y promedio (para Spline de orden 3 en particular) totalmente alejados del resto de modelos, incluso negativos. Se visualizarán estos valores con <i>boxplots</i> y, de mantenerse aislados, se descartarán para la imputación final. Además, en el nuevo <i>dataset</i> se observa que los interpolados con método polinomial de orden 2 y 3, así como con <i>backward fill</i> cuentan aún con datos nulos, es decir, aquello que se desea sustituir, debido a la naturaleza de sus algoritmos. Por ello, quedan nuevamente descartados.
</h4>
"""

cols = df[['PM2.5'] + [col for col in df.columns if 'PM2.5' in col and not df[col].isnull().any()]]
fig = make_subplots(rows = 1, cols = len(cols.columns))

for i, col in enumerate(cols.columns):
  fig.add_trace(go.Box(y = df[col], name = col), row=1, col = i+1)

fig.update_layout(height=600, width=1350, title_x = 0.5, title_text="Boxplots de las variables", template='plotly_dark')
fig.show()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">

<h3><b><i>Insights</i></b></h3>
<h4>
Los resultados de la interpolación usando <i>Spline</i>, tanto para orden 2 como para orden 3, han arrojado una cantidad inadmisible de <i>outliers</i> o valores atípicos. Por ello, se descartarán y se tomarán en cuenta únicamente los otros modelos con tendencias y periodos de estacionalidad significativos que se visualizarán obteniendo el cambio en la cantidad de partículas PM 2.5 con respecto a periodos anteriores que mantengan un comportamiento lo más parecido posible al de los registros originales de partículas PM 2.5 sin imputación. Cabe destacar que se descartan igualmente métodos que siguen contando con valores faltantes, como el <i>backward fill</i> dada su naturaleza. Posteriormente, se visualizará el Error Cuadrático Medio (MSE, por sus siglas en inglés - <i>Mean Squared Error</i>) y, con base en ello, serán imputados los resultados de dicho modelo en aquellos valores faltantes de la columna PM 2.5 original. A continuación, se obtiene y visualiza el cambio con respecto a periodos anteriores de los métodos de imputación restantes, rechazando aquellos que se mantengan sin variación en los periodos faltantes al no coincidir con la tendencia y estacionalidad de la serie original de PM 2.5
</h4>
"""

new_cols = []
for col in cols.columns:
  if 'SP3' not in col and 'SP2' not in col:
    df['Change_'+col] = df[col].div(df[col].shift())
    new_cols += ['Change_'+col]
pd.set_option('display.float_format', lambda x: '%.4f' % x)
df.head(20)

fig = px.line(df.loc[df['Date']>'2012'], x='Date', y=new_cols, template='plotly_dark',
              labels={'value': 'Concentración de PM2.5', 'Date': 'Fecha'}, title='<b>Cambio en la cantidad de partículas PM2.5 en MTY por Imputación</b>')

# Personalización de las líneas
fig.update_traces(selector=dict(name='Change_PM2.5'), line=dict(color='lightgreen', dash='solid', width=4), name='PM2.5 Original')

colors = ['red', 'blue', 'purple', 'magenta', 'lightblue', 'white']
names = ['PM2.5 - Regresión Preliminar', 'PM2.5 Interpolado Lineal', 'PM2.5 Forward Filling', 'PM2.5 - Media Móvil', 'PM2.5 ES Aditivo',
         'PM2.5 ES Multiplicativo']
dashes = ['dash', 'dot', 'dashdot', 'longdash', 'longdashdot', 'dash']

for col, color, name, dash in zip(new_cols[1:], colors, names, dashes):
  fig.update_traces(selector=dict(name=col), line=dict(color=color, dash=dash), name=name)

# Ajuste de la leyenda
fig.update_layout(legend_title_text='Cambio en Series de PM2.5', title_x=0.5)

# Mostrar la figura
fig.show()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">

<h3><b><i>Insights</i></b></h3>
<h4>
En este gráfico utilizamos la linea verde que muestra la serie original como una referencia para evaluar la precisión de cada método de imputación. Podemos observar que la interpolación lineal y la media móvil son las que mas se acercan a esa serie original. Comparten las mismas tendencias y cambios en la grafica.
El método de regresión preliminar muestra mucha variabilidad y se desalinea mucho de la original en ciertos puntos, principalmente de enero a agosto de 2013 y agosto a diciembre de 2015. El método de forward filling igual se ve que sigue la tendencia y los cambios de la serie original, aunque tiene unas inconsistencias que podrían afectar a nuestro modelo predictivo.
Métodos como el de media móvil pueden parecer muy bien pero al suavizar demasiado pueden ocultar ciertos patrones críticos, mientras que la interpolación lineal alcanza un buen equilibro entre la precisión y la suavidad. Depende de lo que se busque en el problema se puede elegir un diferente modelo, si se busca un análisis mas general, opciones que capturen la variabilidad o que suavicen mas, podrían ser una buena opción. Mientras que otras opciones como serian la interpolación lineal y el forward filling serian opciones mas equilibradas para mantener la misma estructura de los datos originales.
</h4>
"""

columns = ['PM2.5'] + [col for col in df.columns if 'SP3' not in col and 'SP2' not in col and not df[col].isnull().any()]
df = df[columns]

fig = px.line(df, x='Date', y=['PM2.5', 'PM2.5_IL', 'PM2.5_FF', 'PM2.5 Media Móvil', 'PM2.5_ES+', 'PM2.5_ES*', 'PM2.5_PR'], template='plotly_dark',
              labels={'value': 'Concentración de PM2.5', 'Date': 'Fecha'},
              title='<b>Visualización de Imputación de Datos de Partículas PM 2.5 en MTY con Métodos Variados (2006 - 2013)</b>')
y_cols = ['PM2.5_IL', 'PM2.5_FF', 'PM2.5 Media Móvil', 'PM2.5_ES+', 'PM2.5_ES*', 'PM2.5_PR']

# Personalización de las líneas
fig.update_traces(line=dict(width=2), opacity=0.8)
fig.update_traces(selector=dict(name='PM2.5'), line=dict(color='white', width=3), name='PM2.5 Original')

colors = ['blue', 'magenta', 'red', 'cyan', 'yellow', 'lightblue']
dashes = ['dash', 'dot', 'dashdot', 'longdash', 'longdashdot', 'dot']
names = ['PM2.5 Interpolado Lineal', 'PM2.5 Forward Filling', 'PM2.5 - Media Móvil', 'PM2.5 ES Aditivo', 'PM2.5 ES Multiplicativo',
         'PM2.5 Regresión Preliminar']

for col, color, dash, name in zip([c for c in y_cols if 'PM2.5' in c and c != 'PM2.5' and 'Change' not in c], colors, dashes, names):
  fig.update_traces(selector=dict(name=col), line=dict(color=color, dash=dash), name=name)

# Ajuste de la leyenda
fig.update_layout(legend_title_text='Series de PM2.5', title_x=0.5)

# Mostrar la figura
fig.show()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">

<h3><b><i>Insights</i></b></h3>
<h4>
Se visualizan los métodos de imputación restantes y se comparan con la serie original. Por medio de la gráfica se aprecia el nulo cambio en los periodos faltantes de PM 2.5 con el método de <i>forward filling</i> dada su naturaleza, mientras que, por la misma razón junto con la irregularidad en los intervalos de la serie original, el interpolado lineal luce poco creíble, en especial para un periodo de 6 meses, por lo que queda descartado y se consideran únicamente los métodos de predicción con media móvil, suavizamiento exponencial aditivo y multiplicativo y la regresión preliminar para datos faltantes.
</h4>
"""

# Datos de los MSEs para modelos a usar
mse_results = {}

# Obtención de valores del ES aditivo con MSE
es_model = ES(df['PM2.5'].dropna(), trend='add', seasonal='add', seasonal_periods=12).fit()
es_pred = es_model.predict(start=0, end=len(df['PM2.5'].dropna()) - 1)
mse_results['ES Aditivo'] = mean_squared_error(
    df['PM2.5'].dropna(),  # Datos originales no NaN
    es_pred  # Predicciones ES+ para datos originales
)

# Obtención de valores del ES multiplicativo con MSE
es_model = ES(df['PM2.5'].dropna(), trend='mul', seasonal='mul', seasonal_periods=12).fit()
es_pred = es_model.predict(start=0, end=len(df['PM2.5'].dropna()) - 1)
mse_results['ES Multiplicativo'] = mean_squared_error(
    df['PM2.5'].dropna(),  # Datos originales no NaN
    es_pred  # Predicciones ES* para datos originales
)

# Obtención de valores de predicción con MA con MSE
mse_results['Media Móvil'] = mean_squared_error(
    df['PM2.5'].dropna(), # Datos originales no NaN
    df['PM2.5'].dropna().rolling(window = 12, min_periods = 1).mean()  # Predicciones MA para datos originales
)

# Visualización con un dataframe ordenado, junto con su RMSE
mse_results['Regresión Lineal'] = mse_test # MSE de la regresión preliminar
mse_df = pd.DataFrame(list(mse_results.items()), columns=['Método', 'MSE'])
mse_df['RMSE'] = np.sqrt(mse_df['MSE'])
mse_df.sort_values(by='MSE')

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">

<h3><b><i>Insights</i></b></h3>
<h4>
El modelo con menor MSE para la imputación de datos es la regresión preliminar. Es ideal usarlo para la imputación de datos. Se conservan los datos no faltantes originales, mientras que los datos faltantes serán reemplazados por las predicciones del modelo de regresión preliminar. Se realizará una prueba de hipótesis de la significancia del modelo para asegurarnos de que sus resultados son confiables. De rechazarse la hipótesis, se mantendrán los resultados obtenidos con la imputación del suavizamiento exponencial multiplicativo por ser el segundo método de imputación con menor error aparte de la regresión preliminar.</h4>
"""

# Observaciones y predicciones
df = df_combined.copy()
n = len(df_test_Y)

k = df_test_X.shape[1]  # Variables independientes
u = k
v = n - (k + 1)

# Modelo
model = LinearRegression()
model.fit(df_test_X, df_test_Y)

# Predicciones y residuos
y_pred = model.predict(df_test_X)

# Predicciones y residuos
residuals = df_test_Y - y_pred

# Suma de cuadrados
SSR = np.sum((y_pred - np.mean(df_test_Y))**2)
SSE = np.sum(residuals**2)
SST = SSR + SSE

# Errores promedio
MSR = SSR / k
MSE = SSE / v

# R2
R2 = model.score(df_test_X, df_test_Y)

# F-Stat
F_cal = MSR / MSE

a = 0.05

# Valor crítico de F
F_critical = ss.f.ppf(1 - a, u, v)

# p-valor de F-Stat
p_value_F = 1 - ss.f.cdf(F_cal, u, v)

# Prueba Global del Modelo de Regresión Lineal Múltiple para la Imputación de NaNs en PM2.5
print(f"R^2 del modelo de regresión lineal múltiple: {R2:.4f}")
print(f"Estadístico F asociado al modelo: {F_cal:.4f}")
print(f"Valor crítico de F (significancia = 0.05): {F_critical:.4f}")
print(f"p-value de F: {p_value_F:.4f}\n")
# Resultado de Prueba de hipótesis
if p_value_F < a:
  print('Se rechaza H0. El modelo es estadísticamente significativo.')
else:
  print('No se rechaza H0. El modelo no es estadísticamente significativo.')

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b><i>Insights</i></b></h3>
<h4>
Tras ser elegido el modelo preliminar de regresión múltiple para datos faltantes, se observa que se rechaza nuestra hipótesis nula, es decir, que el modelo no es significativo. Por lo anterior, no hay evidencia para rechazar los datos que ha arrojado que, de acuerdo con nuestros estadísticos calculados tanto para el F asociado al modelo como para su p valor, los resultados del modelo son creíbles. A continuación se realizará, inicialmente, la regresión múltiple para los datos tanto con PCA como sin PCA. Para este último, se tomará en cuenta una posible multicolinealidad visualizada, o no, en una matriz de correlación entre los contaminantes y las partículas PM 2.5 y, de presentarse alguna, o algunas, que den problemas, serán reducidas mediante PCA, ya que la pérdida de información no será significativa y, de ser así, será comparado con el modelo sin PCA para elegir uno final.</h4>
<h2><b>Análisis de Datos</b></h2>
<h3><b>Implementación del Modelo de Regresión Lineal Múltiple</b></h3>
<h4><b>Modelo Sin Análisis de Componentes Principales (PCA)</b></h4>
"""

# Visualización de la Serie sin Datos Faltantes
fig = px.line(df, x='Date', y='PM2.5', title='<b>Serie de Tiempo de Partículas PM 2.5 en MTY (2006 - 2013)</b>',
              color_discrete_sequence=['yellow'], template='plotly_dark', labels={'PM2.5': 'Concentración de PM 2.5', 'Date': 'Fecha'})
# Ajuste de la leyenda
fig.update_layout(title_x=0.5)
fig.show()

fig = make_subplots(rows = 1, cols = len(df.columns[1 : ]))

for i, col in enumerate(df.columns[1:]):
  fig.add_trace(go.Box(y = df[col], name = col), row=1, col = i+1)

fig.update_layout(height=600, width=1200, title_x = 0.5, title_text="<b>Boxplots de las variables</b>", template='plotly_dark')
fig.show()

# Inspección visual linealidad
fig = make_subplots(rows=6, cols=2, subplot_titles=df.columns[1:])

for i, col in enumerate(df.columns[1:]):
    row = (i // 2) + 1
    col_plot = (i % 2) + 1

    X = sm.add_constant(df[col])
    model = sm.OLS(df['PM2.5'], X).fit()
    predictions = model.predict(X)

    fig.add_trace(go.Scatter(x=df[col], y=df['PM2.5'], mode='markers', name=col), row=row, col=col_plot)
    fig.add_trace(go.Scatter(x=df[col], y=predictions, mode='lines', name=col), row=row, col=col_plot)

fig.update_layout(height=1200, width=1300, title_text="<b>Variables independientes vs PM2.5</b>", template='plotly_dark')
fig.show()

corr = df.drop(['Date'], axis=1).corr()

# Matriz sin repeticiones (triángulo inferior)
df_lt = corr.where(np.tril(np.ones(corr.shape)).astype(bool))

# Convertir la matriz a formato de texto, redondear y reemplazar ceros con cadenas vacías
df_lt_text = df_lt.applymap(lambda x: f'{x:.4f}' if pd.notna(x) and x != 0 else '')

# Crear la gráfica de calor
fig = px.imshow(df_lt, text_auto=False, aspect="auto", color_continuous_scale='haline_r', zmin=-1, zmax=1,
                template='plotly_dark', height=600, width=1200)

# Añadir un trazo alrededor de cada celda
fig.update_traces(hovertemplate=None, hoverinfo='skip', selector=dict(type='heatmap'), xgap=1, ygap=1, colorbar=dict(title='Value'))

# Añadir texto personalizado a las celdas
fig.update_traces(text=df_lt_text.values, texttemplate='%{text}', textfont=dict(size=12))

# Configurar el título y las etiquetas
fig.update_layout(title='<b>Matriz de Correlación de las Variables para la Regresión</b>', title_x=0.5, xaxis_title='Variable', yaxis_title='Variable')

# Mostrar la gráfica
fig.show()

# Pruebas de hipótesis de los coeficientes de correlación con PM 2.5
from scipy.stats import pearsonr

for col in df.drop(['Date'], axis=1).corr().columns:
  corr_coef, p_value = pearsonr(df['PM2.5'], df[col])
  print(f"Correlación entre PM2.5 y {col}:")
  print(f"Coeficiente: {corr_coef}")
  print(f"P-valor: {p_value}")
  if p_value < 0.05:
    print("La correlación es estadísticamente significativa. ✔️​​\n")
  else:
    print("La correlación NO es estadísticamente significativa. ​❌​​\n")

# Separamos las variables independientes y
X = df.drop(['Date', 'PM2.5'], axis=1)
y = df['PM2.5']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)

model = LinearRegression().fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)
r2_adj_train = 1 - (1 - r2_train) * (len(y_train) - 1) / (len(y_train) - X_train.shape[1] - 1)
r2_adj_test = 1 - (1 - r2_test) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

print(f"R^2 ajustado para el entrenamiento: {r2_adj_train:.4f}")
print(f"R^2 ajustado para la prueba: {r2_adj_test:.4f}")

fig = go.Figure()
fig.add_trace(go.Scatter(x = df.index, y=df['PM2.5'], mode='lines', name='PM2.5', line=dict(color='white')))
fig.add_trace(go.Scatter(x = X_train.index, y=y_train_pred, mode='lines', name='Predicción PM2.5 (Train)', line=dict(color='red')))
fig.add_trace(go.Scatter(x = X_test.index, y=y_test_pred, mode='lines', name='Predicción PM2.5 (Test)', line=dict(color='blue')))

fig.update_layout(title='<b>Regresión Lineal Múltiple para PM2.5</b>', xaxis_title='Fecha', yaxis_title='PM2.5', template='plotly_dark', title_x=0.5,
                  legend_title_text='Datos vs. Modelo')
fig.show()

y_pred_combined = np.concatenate([y_train_pred, y_test_pred])
y_original = np.concatenate([y_train, y_test])

n = len(y_pred_combined)

k = X_test.shape[1]  # Variables independientes
u = k
v = n - (k + 1)

# Predicciones y residuos
residuals = y_original - y_pred_combined

# Suma de cuadrados
SSR = np.sum((y_pred_combined - np.mean(y_original))**2)
SSE = np.sum(residuals**2)
SST = SSR + SSE

# Errores promedio
MSR = SSR / k
MSE = SSE / v

# F-Stat
F_cal = MSR / MSE

a = 0.05

# Valor crítico de F
F_critical = ss.f.ppf(1 - a, u, v)

# p-valor de F-Stat
p_value_F = 1 - ss.f.cdf(F_cal, u, v)

# Prueba Global del Modelo de Regresión Lineal Múltiple para la Imputación de NaNs en PM2.5
print(f"R^2 del modelo de regresión lineal múltiple: {r2_score(y_original, y_pred_combined):.4f}")

print(f"Estadístico F asociado al modelo: {F_cal:.4f}")

print(f"Valor crítico de F (significancia = 0.05): {F_critical:.4f}\n")

print(f"p-value de F (Entrenamiento): {p_value_F:.4f}")

# Resultado de Prueba de hipótesis
if p_value_F < a:
  print('Se rechaza H0. El modelo es estadísticamente significativo.')
else:
  print('No se rechaza H0. El modelo es estadísticamente significativo.')

from sklearn.model_selection import TimeSeriesSplit

# Definimos el número de divisiones
tscv = TimeSeriesSplit(n_splits=5)

scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')
scores_adj = 1 - (1 - scores) * (len(y) - 1) / (len(y) - X.shape[1] - 1)
print("Valores de R^2 ajustado de la validación cruzada:", scores_adj)
print("Promedio de los valores de R^2 ajsutado:", scores_adj.mean())

# Calcular la varianza de los residuos
residual_sum_of_squares = np.sum(residuals**2)
degrees_of_freedom = len(y) - len(model.coef_) - 1
residual_variance = residual_sum_of_squares / degrees_of_freedom

# Calcular la varianza de los coeficientes
X_with_const = np.c_[np.ones(X.shape[0]), X]  # Agregar una columna de unos para el intercepto
cov_matrix = np.linalg.inv(X_with_const.T @ X_with_const) * residual_variance
standard_errors = np.sqrt(np.diag(cov_matrix))

# Calcular los valores t y los valores p
t_values = model.coef_ / standard_errors[1:]  # Excluir el error estándar del intercepto
p_values = [2 * (1 - ss.t.cdf(np.abs(t), degrees_of_freedom)) for t in t_values]

coef_df = pd.DataFrame({'Variable': X.columns, 'Coeficiente': model.coef_,
                        'Error Estándar': standard_errors[1:], 'Valor t': t_values, 'Valor p': p_values})

intercept_df = pd.DataFrame({'Variable': ['Intersección'], 'Coeficiente': [model.intercept_],
                             'Error Estándar': [standard_errors[0]], 'Valor t': [model.intercept_ / standard_errors[0]],
                             'Valor p': [2 * (1 - ss.t.cdf(np.abs(model.intercept_ / standard_errors[0]), degrees_of_freedom))]})

summary_df = pd.concat([coef_df, intercept_df], ignore_index=True)
summary_df['Significativo'] = summary_df['Valor p'] < 0.05
summary_df

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Resultados</b></h3>
<h4>
El primer modelo de regresión lineal múltiple aplicado para predecir los niveles de PM2.5 mostró un desempeño decente en el conjunto de entrenamiento, con un R<sup>2</sup> ajustado de 0.6979, lo que sugiere que el modelo es capaz de explicar una porción considerable de la variabilidad en los datos de entrenamiento. Sin embargo, en el conjunto de prueba, el R<sup>2</sup> ajustado fue negativo (-0.1155), indicando que el modelo no generalizó bien, y que podría estar sobreajustado a los datos de entrenamiento. Además, la validación cruzada utilizando <i>TimeSeriesSplit</i> con 5 divisiones reveló una variabilidad considerable en los valores de R<sup>2</sup> ajustado entre las particiones, con un promedio de -0.2694, lo que confirma que el modelo no se desempeñó consistentemente a través de los diferentes subconjuntos temporales. Esta variabilidad resalta la importancia de la validación cruzada en series temporales, ya que permite evaluar cómo el modelo podría comportarse en futuros datos no vistos y cómo la selección del número de divisiones puede influir en los resultados finales. A pesar de lo anterior, la prueba de hipótesis realizada indica que el modelo es estadísticamente significativo y, aún así, sus resultados pueden ser confiables.</h4>
<h3><b>Implementación del Análisis de Conglomerados con <i>K-Means</i></b></h3>
<h4><b>Modelo Sin Análisis de Componentes Principales (PCA)</b></h4>
"""

# Seleccionamos columnas numéricas
X = df.drop(columns=['Date'])

# Estandarización
X_scaled = StandardScaler().fit_transform(X)

# Rango de K
range_n_clusters = range(2, 11)

# Coeficientes de silueta
silhouette_scores = []
for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Se organizan los resultados del coeficiente
silhouette_df = pd.DataFrame({'n_clusters': range_n_clusters, 'silhouette_score': silhouette_scores})

# Se grafican los coeficientes
fig = px.line(silhouette_df, x='n_clusters', y='silhouette_score',
              title='<b>Coeficiente de la Silueta para Distintos Clusters</b>',
              labels={'n_clusters': 'Number of Clusters', 'silhouette_score': 'Silhouette Score'}, template='plotly_dark', color_discrete_sequence=['blue'])
fig.update_layout(title_x=0.5)
fig.show()

# Aplicamos KMeans con k = 2
kmeans = KMeans(n_clusters=2, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# Add cluster labels to the DataFrame
df['cluster'] = cluster_labels

# Reducir la dimensionalidad a 3 componentes principales usando PCA
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

# Crear un DataFrame con los componentes principales y las etiquetas de cluster
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])
df_pca['cluster'] = cluster_labels

# Graficamos los datos
fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3',
                    color='cluster', title='<b>Agrupamiento para K = 2 Con PCA para Visualización</b>', template='plotly_dark', width=800)
fig.update_layout(title_x=0.5)
fig.show()

# Inspección visual linealidad
fig = make_subplots(rows=6, cols=2, subplot_titles=df.columns[1:])
cols = [col for col in df.columns if col not in ['cluster', 'Date']]

# Colores por clúster
cluster_colors = {0: 'blue', 1: 'yellow'}

for i, col in enumerate(df.columns[1:len(df.columns)-1]):
  row = (i // 2) + 1
  col_plot = (i % 2) + 1

  fig.add_trace(go.Scatter(x=df[col], y=df['PM2.5'], mode='markers', marker_color=df['cluster'], showlegend=False), row=row, col=col_plot)

fig.update_layout(height=1000, width=1300, title_text="<b>Predictores vs PM2.5 por Cluster</b>", title_x=0.5, template='plotly_dark')
fig.show()

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
num_cols = list(df.select_dtypes(include=numerics).columns)
df.groupby('cluster')[num_cols].mean()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Resultados</b></h3>
<h4>
En el análisis de <i>K-Means</i> con K = 2 y sin aplicar PCA, los resultados revelaron dos grupos distintos, con un coeficiente de silueta de 0.29, lo que indica una separación moderadamente débil entre los clústeres. Al observar las variables involucradas, uno de los grupos podría estar asociado con niveles elevados de contaminantes como NO2, NOX, y PM10, lo cual sugiere zonas o periodos de alta contaminación, mientras que el otro grupo muestra niveles relativamente más bajos, sugiriendo condiciones de menor contaminación. Estos grupos podrían etiquetarse como "Alta Contaminación" y "Baja Contaminación".</h4>
<h3><b>Implementación de Modelo de Regresión Lineal Múltiple</b></h3>
<h4><b>Modelo Con Análisis de Componentes Principales (PCA)</b></h4>
"""

# Cálculo del factor de inflación de la varianza. Renombrando una columna
df_temp = df.rename(columns={'PM2.5': 'PM2_5'})

formula = 'PM2_5 ~ CO + NO + NO2 + NOX + O3 + PM10 + PRS + RH + SR + TOUT'
y, X = dmatrices(formula, df_temp, return_type='dataframe')

vif = pd.DataFrame()
vif['Variable'] = X.columns
vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Revertir el nombre de la columna
df_temp = df_temp.rename(columns={'PM2_5': 'PM2.5'})
vif.sort_values(by='VIF', ascending=False)

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b><i>Insights</i></b></h3>
<h4>
Con VIFs extremadamente altos, las variables NOX, NO2, y NO están altamente correlacionadas entre sí, lo que podría provocar problemas para los modelos anteriores. En su lugar, y como se comentó en la sección de metodología, se opta por eliminar NOX y NO2 para reducir la redundancia. Este paso es importante para evitar que las variables altamente correlacionadas dominen los primeros componentes principales en el PCA.<br>A continuación, es necesario estandarizar las variables restantes (es decir, escalarlas para que tengan media 0 y desviación estándar 1), como se hizo en la implementación de <i>K-Means</i> sin PCA, porque el PCA es sensible a las diferencias en escala.</h4>
"""

# Dataset reducido
# df_reduced = df.drop(columns=['NOX', 'NO2'])

# Se separan las variables independientes de la variable objetivo (PM2.5) para la regresión (si es KMeans, no se descarta PM 2.5)
X = df.drop(columns=['Date', 'PM2.5'])

# Se estandarizan las variables independientes
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X[['NO', 'NO2', 'NOX']])

# Se aplica PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Varianza explicada acumulada
var_exp_cumulative = pca.explained_variance_ratio_.cumsum()

# Visualización
fig = px.line(x=range(1, len(var_exp_cumulative)+1), y=var_exp_cumulative, markers=True,
              labels={'x': 'Número de Componentes', 'y': 'Varianza Explicada Acumulada'},
              title='<b>Varianza Explicada por los Componentes Principales</b>', template='plotly_dark')

fig.update_traces(line=dict(color='cyan'), marker=dict(color='white'))
fig.update_layout(title_x=0.5)

fig.show()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b><i>Insights</i></b></h3>
<h4>
Para cumplir con la función de reducir dimensiones y evitar la multicolinealidad, se elige trabajar con 2 componentes principales, ya que estos explican el 99% de la varianza acumulada y permiten deshacerse de una cantidad importante de variables.</h4>
"""

# Retener suficientes componentes para capturar el 95% de la varianza
n_components = 2
pca = PCA(n_components=n_components)
X_pca_final = pca.fit_transform(X_scaled)

# Crear un DataFrame con los componentes principales
df_pca = pd.DataFrame(data=X_pca_final, columns=[f'PC{i+1}' for i in range(n_components)])
df_pca['PM2.5'] = df['PM2.5']
df_pca['Date'] = df['Date']
df_pca

pca.explained_variance_ratio_

fig = make_subplots(rows = 1, cols = len(df_pca.drop(['Date'], axis=1).columns))

for i, col in enumerate(df_pca.drop(['Date'], axis=1).columns):
  fig.add_trace(go.Box(y = df_pca[col], name = col), row=1, col = i+1)

fig.update_layout(height=600, width=1200, title_x = 0.5, title_text="<b>Boxplots de los Componentes Principales y PM 2.5</b>", template='plotly_dark')
fig.show()

fig = px.scatter(df_pca, x='PC1', y='PC2', color='PM2.5', title='<b>Visualización de partículas PM 2.5 con 3/5 PCAs</b>',
                    template='plotly_dark', color_continuous_scale='haline_r', width=600)
fig.update_layout(title_x=0.5)
fig.show()

# Inspección visual linealidad
fig = make_subplots(rows=1, cols=3, subplot_titles=df_pca.drop(['Date'], axis=1).columns)

for i, col in enumerate(df_pca.drop(['Date'], axis=1).columns):
    row = 1
    col_plot = (i % 3) + 1

    X = sm.add_constant(df_pca[col])
    model = sm.OLS(df_pca['PM2.5'], X).fit()
    predictions = model.predict(X)

    fig.add_trace(go.Scatter(x=df_pca[col], y=df_pca['PM2.5'], mode='markers', name=col), row=row, col=col_plot)
    fig.add_trace(go.Scatter(x=df_pca[col], y=predictions, mode='lines', name=col), row=row, col=col_plot)

fig.update_layout(height=500, width=1200, title_text="<b>Componentes Principales vs PM2.5</b>", template='plotly_dark', title_x=0.5)
fig.show()

corr = df_pca.drop(['Date'], axis=1).corr()

# Matriz sin repeticiones (triángulo inferior)
df_lt = corr.where(np.tril(np.ones(corr.shape)).astype(bool))

# Convertir la matriz a formato de texto, redondear y reemplazar ceros con cadenas vacías
df_lt_text = df_lt.applymap(lambda x: f'{x:.4f}' if pd.notna(x) and x != 0 else '')

# Crear la gráfica de calor
fig = px.imshow(df_lt, text_auto=False, aspect="auto", color_continuous_scale='haline_r', zmin=-1, zmax=1,
                template='plotly_dark', height=600, width=1100)

# Añadir un trazo alrededor de cada celda
fig.update_traces(hovertemplate=None, hoverinfo='skip', selector=dict(type='heatmap'), xgap=1, ygap=1, colorbar=dict(title='Value'))

# Añadir texto personalizado a las celdas
fig.update_traces(text=df_lt_text.values, texttemplate='%{text}', textfont=dict(size=12))

# Configurar el título y las etiquetas
fig.update_layout(title='<b>Matriz de Correlación de las 5 PCAs para la Regresión</b>', title_x=0.5, xaxis_title='PCA / PM 2.5', yaxis_title='PCA / PM 2.5')

# Mostrar la gráfica
fig.show()

# Pruebas de hipótesis de los coeficientes de correlación con PM 2.5
for col in df_pca.drop(['Date'], axis=1).corr().columns:
  corr_coef, p_value = pearsonr(df_pca['PM2.5'], df_pca[col])
  print(f"Correlación entre PM2.5 y {col}:")
  print(f"Coeficiente: {corr_coef}")
  print(f"P-valor: {p_value}")
  if p_value < 0.05:
    print("La correlación es estadísticamente significativa. ✔️​​\n")
  else:
    print("La correlación NO es estadísticamente significativa. ​❌​​\n")

# Separamos las variables independientes y
X = df_pca.drop(['Date', 'PM2.5'], axis=1)
y = df_pca['PM2.5']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)

model = LinearRegression().fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)
r2_adj_train = 1 - (1 - r2_train) * (len(y_train) - 1) / (len(y_train) - X_train.shape[1] - 1)
r2_adj_test = 1 - (1 - r2_test) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

print(f"R^2 ajustado para el entrenamiento (con PCA): {r2_adj_train:.4f}")
print(f"R^2 ajustado para la prueba (con PCA): {r2_adj_test:.4f}")

fig = go.Figure()
fig.add_trace(go.Scatter(x = df_pca.index, y=df_pca['PM2.5'], mode='lines', name='PM2.5', line=dict(color='white')))
fig.add_trace(go.Scatter(x = X_train.index, y=y_train_pred, mode='lines', name='Predicción PM2.5 (Train)', line=dict(color='red')))
fig.add_trace(go.Scatter(x = X_test.index, y=y_test_pred, mode='lines', name='Predicción PM2.5 (Test)', line=dict(color='blue')))

fig.update_layout(title='<b>Regresión Lineal Múltiple para PM2.5 con Análisis de Componentes Principales</b>', xaxis_title='Fecha', yaxis_title='PM2.5',
                  template='plotly_dark', title_x=0.5, legend_title_text='Valores')
fig.show()

y_pred_combined = np.concatenate([y_train_pred, y_test_pred])
y_original = np.concatenate([y_train, y_test])

n = len(y_pred_combined)

k = X_test.shape[1]  # Variables independientes
u = k
v = n - (k + 1)

# Predicciones y residuos
residuals = y_original - y_pred_combined

# Suma de cuadrados
SSR = np.sum((y_pred_combined - np.mean(y_original))**2)
SSE = np.sum(residuals**2)
SST = SSR + SSE

# Errores promedio
MSR = SSR / k
MSE = SSE / v

# F-Stat
F_cal = MSR / MSE

a = 0.05

# Valor crítico de F
F_critical = ss.f.ppf(1 - a, u, v)

# p-valor de F-Stat
p_value_F = 1 - ss.f.cdf(F_cal, u, v)

# Prueba Global del Modelo de Regresión Lineal Múltiple para la Imputación de NaNs en PM2.5
print(f"R^2 del modelo de regresión lineal múltiple: {r2_score(y_original, y_pred_combined):.4f}")

print(f"Estadístico F asociado al modelo: {F_cal:.4f}")

print(f"Valor crítico de F (significancia = 0.05): {F_critical:.4f}\n")

print(f"p-value de F (Entrenamiento): {p_value_F:.4f}")

# Resultado de Prueba de hipótesis
if p_value_F < a:
  print('Se rechaza H0. El modelo es estadísticamente significativo.')
else:
  print('No se rechaza H0. El modelo es estadísticamente significativo.')

# Definimos el número de divisiones
tscv = TimeSeriesSplit(n_splits=5)

scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')
scores_adj = 1 - (1 - scores) * (len(y) - 1) / (len(y) - X.shape[1] - 1)
print("Valores de R^2 ajustado de la validación cruzada:", scores_adj)
print("Promedio de los valores de R^2 ajsutado:", scores_adj.mean())

# Calcular la varianza de los residuos
residual_sum_of_squares = np.sum(residuals**2)
degrees_of_freedom = len(y) - len(model.coef_) - 1
residual_variance = residual_sum_of_squares / degrees_of_freedom

# Calcular la varianza de los coeficientes
X_with_const = np.c_[np.ones(X.shape[0]), X]  # Agregar una columna de unos para el intercepto
cov_matrix = np.linalg.inv(X_with_const.T @ X_with_const) * residual_variance
standard_errors = np.sqrt(np.diag(cov_matrix))

# Calcular los valores t y los valores p
t_values = model.coef_ / standard_errors[1:]  # Excluir el error estándar del intercepto
p_values = [2 * (1 - ss.t.cdf(np.abs(t), degrees_of_freedom)) for t in t_values]

coef_df = pd.DataFrame({'Variable': X.columns, 'Coeficiente': model.coef_,
                        'Error Estándar': standard_errors[1:], 'Valor t': t_values, 'Valor p': p_values})

intercept_df = pd.DataFrame({'Variable': ['Intersección'], 'Coeficiente': [model.intercept_],
                             'Error Estándar': [standard_errors[0]], 'Valor t': [model.intercept_ / standard_errors[0]],
                             'Valor p': [2 * (1 - ss.t.cdf(np.abs(model.intercept_ / standard_errors[0]), degrees_of_freedom))]})

summary_df = pd.concat([coef_df, intercept_df], ignore_index=True)
summary_df['Significativo'] = summary_df['Valor p'] < 0.05
summary_df

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Resultados</b></h3>
<h4>
Aunque la regresión lineal múltiple sin PCA también muestra un R<sup>2</sup> ajustado bajo (0.2483), la pérdida de información al aplicar PCA es notable. Esto se refleja en la menor capacidad explicativa del modelo con PCA. A pesar de que ambos modelos son estadísticamente significativos según el p-valor del estadístico F, el modelo sin PCA mantiene la estructura original de los datos, preservando más información, lo que resulta en un desempeño relativamente mejor.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Implementación del Análisis de Conglomerados con <i>K-Means</i></b></h3>
<h4><b>Modelo Con Análisis de Componentes Principales (PCA)</b></h4>
"""

# Seleccionamos columnas numéricas
X = df_pca.drop(columns=['Date'])

# Estandarización del PCA
X_scaled = StandardScaler().fit_transform(X)

# Rango de K
range_n_clusters = range(2, 11)

# Coeficientes de silueta
silhouette_scores = []
for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Se organizan los resultados del coeficiente
silhouette_df = pd.DataFrame({'n_clusters': range_n_clusters, 'silhouette_score': silhouette_scores})

# Se grafican los coeficientes
fig = px.line(silhouette_df, x='n_clusters', y='silhouette_score',
              title='<b>Coeficiente de la Silueta para Distintos Clusters</b>',
              labels={'n_clusters': 'Number of Clusters', 'silhouette_score': 'Silhouette Score'}, template='plotly_dark', color_discrete_sequence=['blue'])
fig.update_layout(title_x=0.5)
fig.show()

# Apply KMeans with k = 3
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# Add cluster labels to the DataFrame
df_pca['cluster'] = cluster_labels

# Graficamos los datos
fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PM2.5',
                    color='cluster', title='<b>Agrupamiento para K = 3 Con PCA</b>', template='plotly_dark', width=800)
fig.update_layout(title_x=0.5)
fig.show()

# Inspección visual linealidad
fig = make_subplots(rows=1, cols=2, subplot_titles=df_pca.drop(['Date'], axis=1).columns)

# Colores por clúster
cluster_colors = {0: 'blue', 1: 'yellow', 2: 'red'}

for i, col in enumerate(df_pca.drop(['Date'], axis=1).columns):
  row = 1
  col_plot = (i % 2) + 1

  fig.add_trace(go.Scatter(x=df_pca[col], y=df_pca['PM2.5'], mode='markers', marker_color=df_pca['cluster'], showlegend=False), row=row, col=col_plot)

fig.update_layout(height=500, width=1100, title_text="<b>Predictores vs PM2.5 por Cluster con PCA</b>", title_x=0.5, template='plotly_dark')
fig.show()

num_cols = list(df_pca.select_dtypes(include=numerics).columns)
df_pca.groupby('cluster')[num_cols].mean()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Resultados</b></h3>
<h4>El análisis de <i>K-means</i> utilizando PCA con K = 3, es decir, 3 conglomerados, resultó en un coeficiente de silueta de 0.3687, lo que indica una mejor separación y cohesión de los grupos en comparación con el modelo sin PCA, que obtuvo un coeficiente de 0.29. Esto sugiere que la reducción de dimensionalidad mediante PCA ayudó a identificar clusters más definidos, mejorando la calidad de la agrupación.</h4>

<h2><b>Conclusiones</b></h2>
<h3>El análisis de la contaminación del aire en el Área Metropolitana de Monterrey mediante técnicas estadísticas avanzadas y de aprendizaje automático revela importantes insights sobre los niveles de PM2.5 y sus factores asociados. La regresión lineal múltiple con PCA mostró problemas de sobreajuste, sugiriendo que el PCA puede no ser la mejor estrategia para este conjunto de datos debido a la pérdida de información. Sin embargo, el análisis de <i>K-Means</i> con K = 3 proporcionó una mejor agrupación de los datos en comparación con K = 2. Estos hallazgos destacan la necesidad de explorar técnicas de modelado adicionales y de ajustar los métodos analíticos para mejorar la precisión de las predicciones y la comprensión de los patrones de contaminación en la región.
Para mejorar los resultados, se podrían explorar otros enfoques como la implementación de modelos de machine learning más robustos, como Random Forest o Gradient Boosting, que pueden manejar de mejor manera las relaciones no lineales entre las variables.
</h3>

<h2><b>Referencias</b></h2>
<h3>
<ol>
<li>Aggarwal, S. (2020, mayo 13). Multiple linear Regression. Towards Data Science. Recuperado de <a href = "https://towardsdatascience.com/multiple-linear-regression-8cf3bee21d8b"><i>Multiple Linear Regression</i></a></li>
<li>Aldean, A. S. (2023, junio 29). Time series data interpolation. Medium. Recuperado de <a href = "https://medium.com/@aseafaldean/time-series-data-interpolation-e4296664b86"><i>Time series: data interpolation</i></a></li>
<li>Chatterjee, S. & S. Hadi, A. (2012). <i>Regression Analysis by Example. John Wiley and Sons (WIE)</i>. Recuperado de <a href = "https://sadbhavnapublications.org/research-enrichment-material/2-Statistical-Books/Regression-Analysis-by-Example.pdf"><i>Regression Analysis by Example</i></a></li>
<li>Conceptos básicos sobre el material particulado (PM, por sus siglas en inglés). (2024, junio 25). Agencia de Protección Ambiental de Estados Unidos (EPA). Recuperado de <a href = "https://espanol.epa.gov/espanol/conceptos-basicos-sobre-el-material-particulado-pm-por-sus-siglas-en-ingles">Conceptos básicos sobre partículas PM</a></li>
<li>Contaminación del aire: causas y tipos. (2019, julio 30). Fundación Aquae. Recuperado de <a href = "https://www.fundacionaquae.org/wiki/causas-y-tipos-de-la-contaminacion-del-aire/">Causas y Tipos de la Contaminación del Aire</a></li>
<li><i>Data Science Wizards</i>. (2023, noviembre 3). <i>Preprocessing and data exploration for time series — handling missing values. Medium</i>. Recuperado de <a href = "https://medium.com/@datasciencewizards/preprocessing-and-data-exploration-for-time-series-handling-missing-values-e5c507f6c71c"><i>Preprocessing and data exploration for time series</i></a></li>
<li><i>Fill the gap: EDDI for multivariate time series missing value imputation</i>. (s/f). Techcommunity.microsoft.com. Recuperado el 1 de septiembre de 2024, de <a href = "https://techcommunity.microsoft.com/t5/microsoft-developer-community/fill-the-gap-eddi-for-multivariate-time-series-missing-value/ba-p/3289782"><i>EDDI for multivariate time series missing value imputation</i></a></li>
<li>Gupta, S. (2022, febrero 8). <i>Pre-processing of time series data. EnjoyAlgorithms </i>. Recuperado de <a href = "https://medium.com/enjoy-algorithm/pre-processing-of-time-series-data-c50f8a3e7a98"><i>Pre-processing of time series data</i></a></li>
<li>Hartigan, J. A., & Wong, M. A. (1979). <i>Algorithm AS 136: A K-means clustering algorithm. Journal of the Royal Statistical Society. Series C, Applied statistics </i>, 28(1), 100. Recuperado de <a href = "https://doi.org/10.2307/2346830"><i>A K-means clustering algorithm</i></a></li>
<li>Hernández, F., Usuga, O., & Mazo, M. (2024). Análisis de Regresión con R. Recuperado de <a href = "https://fhernanb.github.io/libro_regresion/">Análisis de Regresión con R</a></li>
<li>Hyndman, R. J., & Athanasopoulos, G. (2018). <i>Forecasting: Principles and Practice. (2nd ed.) OTexts</i>. Recuperado de <a href = "https://otexts.org/fpp2/"><i>Forecasting: Principles and Practice</i></a></li>
<li><i>Imputing time series missing values. (2020, diciembre 18). Crawstat </i>. Recuperado de <a href = "https://crawstat.com/2020/12/18/imputing-time-series-missing-values/"><i>Imputing time series missing values</i></a></li>
<li><i>LinearRegression. (s/f). Scikit-Learn</i>. Recuperado el 1 de septiembre de 2024, de <a href = "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"><i>Sklearn: Linear Regression</i></a></li>
<li>Martínez, M. P. Z. (2023, enero 3). Calidad del aire en Zona Metropolitana: entre regular y mala. Reporte Índigo. Recuperado de <a href = "https://www.reporteindigo.com/reporte/calidad-del-aire-en-zona-metropolitana-entre-regular-y-mala/">Calidad del aire en Zona Metropolitana de MTY</a></li>
<li><i>pandas.DataFrame.bfill — pandas 2.2.2 documentation. (s/f). Pydata.org</i>. Recuperado el 1 de septiembre de 2024, de <a href = "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html"><i>Pandas: BFill</i></a></li>
<li><i>pandas.DataFrame.ffill — pandas 2.2.2 documentation. (s/f). Pydata.org</i>. Recuperado el 1 de septiembre de 2024, de <a href = "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html"><i>Pandas: FFill</i></a></li>
<li><i>pandas.dataframe.interpolate — pandas 2.2.2 documentation. (s/f). Pydata.org</i>. Recuperado el 1 de septiembre de 2024, de <a href = "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html"><i>Pandas: Interpolate</i></a></li>
<li><i>Python</i>. (2019, mayo 23). <i>GeeksforGeeks</i>. Recuperado de <a href = "https://www.geeksforgeeks.org/python-linear-regression-using-sklearn/"><i>Python: Linear Regression using Sklearn</i></a></li>
<li>Sanz, F. (2020, noviembre 26). Algoritmo K-Means Clustering – aplicaciones y desventajas. The Machine Learners. Recuperado de <a href = "https://www.themachinelearners.com/k-means/">Algoritmo K-Means Clustering</a></li>
<li>Seguí, P. (2020, marzo 12). 9 Formas inusuales que la contaminación del aire afecta tu salud. OVACEN. Recuperado de <a href = "https://ovacen.com/contaminacion-aire/">9 Formas inusuales que la contaminación del aire afecta tu salud</a></li>
<li>Snyder, et al. (2017). <i>Contribution of systemic and somatic factors to clinical response and resistance to PD-L1 blockade in urothelial cancer: An exploratory multi-omic analysis. PLoS Medicine</i>, 14(5), e1002309. Recuperado de <a href = "https://doi.org/10.1371/journal.pmed.1002309">Contribution of systemic and somatic factors to clinical response and resistance to PD-L1 blockade in urothelial cancer: An exploratory multi-omic analysis. PLoS Medicine</a></li></li>
<li>Taylor, J. W. (2003). <i>Exponential smoothing with a damped multiplicative trend. International Journal of Forecasting</i>, 19(4), 715–725. Recuperado de <a href = "https://doi.org/10.1016/s0169-2070(03)00003-7">Exponential smoothing with a damped multiplicative trend. International Journal of Forecasting</a></li>
</ol>
</h3>
"""

