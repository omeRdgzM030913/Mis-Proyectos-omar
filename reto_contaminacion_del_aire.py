# -*- coding: utf-8 -*-
"""Reto_Contaminacion_del_Aire.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFQehXpFRyTl79vjIiJ3spiRIVblAPp1

<font face = "Times New Roman" size = "3" color = "black">
<center>
<h3></h3>
<img src = "https://proleon.com.mx/wp-content/uploads/2022/10/01_TM_ESPECIAL_INNOVACION_EDUCATIVA_UNIVERSIDADES_TEC_DE_MONTERREY.png" width = "250">
<h3><b>Campus Estado de M√©xico</h3>
<h3>Aplicaci√≥n de M√©todos Multivariados en Ciencia de Datos</b><br>MA2003B.101 2024-13<br><br><b>Evidencia 2. Contaminaci√≥n del Aire en el √Årea Metropolitana de Monterrey (AMM)<br><br>Omar Rodr√≠guez Montiel A01750836<br><b>Profesores</b><br>Sa√∫l Ju√°rez Ord√≥√±ez<br>Pablo Mendoza Iturralde<br>Faustino Yescas Mart√≠nez</h3>
<img src = "https://images.reporteindigo.com/wp-content/uploads/2023/01/mala-contaminacion-del-aire-monterrey.jpg" width = "500">
<h6>Cr√©dito de la imagen: <a href = "https://www.reporteindigo.com/reporte/calidad-del-aire-en-zona-metropolitana-entre-regular-y-mala/">Calidad del Aire en Zona Metropolitana de Monterrey</a></h6>
</img>
</center>
</font>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Resumen</b></h2>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h4>Este estudio investig√≥ la contaminaci√≥n del aire en el √Årea Metropolitana de Monterrey mediante el an√°lisis de datos de PM2.5 y factores meteorol√≥gicos utilizando t√©cnicas estad√≠sticas y de aprendizaje autom√°tico. Se aplic√≥ regresi√≥n lineal m√∫ltiple y PCA para predecir los niveles de PM2.5, encontrando una correlaci√≥n moderada entre PM2.5 y las dos primeras componentes principales (PC1 y PC2). La regresi√≥n con PCA mostr√≥ un R^2 ajustado de 0.2635 para el entrenamiento y -0.1338 para la prueba, lo que indica sobreajuste en el modelo. El an√°lisis de <i>K-Means</i> con K = 2 present√≥ un coeficiente de silueta de 0.29, mientras que K = 3 mostr√≥ una mejora con un coeficiente de silueta de 0.3687. Estos resultados sugieren que el modelo con K = 3 proporciona una mejor separaci√≥n entre grupos, aunque la t√©cnica de PCA puede haber implicado p√©rdida de informaci√≥n.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b><i>Abstract<i></b></h2>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h4><i>This project investigated air pollution in the Monterrey Metropolitan Area by analyzing PM2.5 data and meteorological factors using statistical and machine learning techniques. Multiple linear regression and PCA were applied to predict PM2.5 levels, finding a moderate correlation between PM2.5 and the first two principal components (PC1 and PC2). The PCA regression showed an adjusted R^2 of 0.2635 for training and -0.1338 for testing, indicating overfitting in the model. K-means analysis with K=2 presented a silhouette coefficient of 0.29, while K=3 showed an improvement with a silhouette coefficient of 0.3687. These results suggest that the model with K=3 provides better separation between groups, although the PCA technique may have implied loss of information.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Palabras clave</b> (<i>keywords</i>)</h2>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h4>Contaminaci√≥n del aire, PM2.5, An√°lisis de Componentes Principales, PCA, <i>K-Means</i>, Regresi√≥n Lineal M√∫ltiple, √Årea Metropolitana de Monterrey, An√°lisis de Datos.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Introducci√≥n</b></h2>
<h4>La contaminaci√≥n del aire es un problema cr√≠tico que afecta la salud p√∫blica y el medio ambiente en √°reas urbanas. En el √Årea Metropolitana de Monterrey (AMM), la calidad del aire ha sido motivo de creciente preocupaci√≥n debido a los altos niveles de contaminantes atmosf√©ricos como PM2.5, que pueden tener efectos adversos en la salud respiratoria y cardiovascular. Este proyecto utiliza t√©cnicas avanzadas de an√°lisis de datos, como el An√°lisis de Componentes Principales (PCA) y el algoritmo <i>K-Means</i>, para examinar la relaci√≥n entre los niveles de PM2.5 y varios factores meteorol√≥gicos, con el objetivo de identificar patrones y mejorar la comprensi√≥n de los factores que contribuyen a la contaminaci√≥n del aire en la regi√≥n.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Descripci√≥n de la problem√°tica</b></h2>
<h4>La contaminaci√≥n del aire en el √Årea Metropolitana de Monterrey (AMM) se ha convertido en un problema serio que impacta tanto la salud p√∫blica como el medio ambiente. La elevada presencia de contaminantes como mon√≥xido de carbono (CO), di√≥xido de nitr√≥geno (NO<sub>2</sub>), di√≥xido de azufre (SO<sub>2</sub>), y part√≠culas PM2.5, causada por actividades humanas tales como la industrializaci√≥n, el transporte y la quema de combustibles f√≥siles, ha llevado a una preocupante reducci√≥n en la calidad del aire en la regi√≥n (Z√∫√±iga Mart√≠nez, 2023). De acuerdo con la Agencia de Protecci√≥n Ambiental de Estados Unidos, las part√≠culas PM2.5 y PM10 son tipos de material particulado en el aire. Las PM10 son part√≠culas inhalables que tienen di√°metros de hasta 10 micr√≥metros, mientras que las PM2.5 son part√≠culas finas m√°s peque√±as, de hasta 2.5 micr√≥metros de di√°metro, lo que las hace 30 veces m√°s peque√±as que un cabello humano promedio. Estas part√≠culas pueden provenir de fuentes directas como obras de construcci√≥n o incendios, o formarse en la atm√≥sfera a partir de reacciones qu√≠micas de contaminantes emitidos por veh√≠culos e industrias.<br><br>Este deterioro se manifiesta no solo en la aparici√≥n de una niebla gris y olores desagradables, sino tambi√©n en un incremento de enfermedades respiratorias y cardiovasculares, as√≠ como en otros efectos adversos sobre la salud de los residentes y la biodiversidad local (Fundaci√≥n Aquae, 2023).<br><br>Para enfrentar este desaf√≠o, es crucial desarrollar herramientas anal√≠ticas avanzadas que permitan comprender las din√°micas de la contaminaci√≥n y su relaci√≥n con factores meteorol√≥gicos. Un enfoque que utilice modelos de regresi√≥n, an√°lisis de conglomerados y an√°lisis de componentes principales podr√≠a ofrecer soluciones clave, facilitando la identificaci√≥n de patrones y la predicci√≥n de niveles de contaminantes como las part√≠culas PM2.5. Adem√°s, el an√°lisis comparativo de la contaminaci√≥n durante per√≠odos de baja circulaci√≥n vehicular, como los experimentados durante la pandemia de COVID-19, podr√≠a abrir la puerta a la implementaci√≥n de pol√≠ticas ambientales m√°s efectivas y mejorar la calidad del aire en el AMM (Segu√≠, P.).</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>Preguntas de investigaci√≥n</b></h2>
<h3><ol type="1"><li>¬øCu√°les son las variables meteorol√≥gicas que tienen mayor impacto en la concentraci√≥n de part√≠culas PM2.5 en el √Årea Metropolitana de Monterrey?</li><li>¬øC√≥mo se agrupan los distintos contaminantes del aire en el AMM y qu√© patrones emergen dentro de estos grupos?</li><li>¬øEs posible predecir con precisi√≥n los niveles de PM2.5 utilizando otros contaminantes y condiciones meteorol√≥gicas como referencia?</li><li>¬øC√≥mo influye la variaci√≥n estacional en la calidad del aire en el AMM y cu√°l es su relaci√≥n con los diferentes contaminantes?</li></ol></h3>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h2><b>M√©todolog√≠a utilizada</b></h2>
<h3><b>Pre-procesamiento</h3></b><h4>En el preprocesamiento de los datos de la serie temporal que contiene las cantidades de contaminantes y part√≠culas PM10 y PM2.5, es fundamental abordar primero la estructuraci√≥n de las marcas de tiempo. Esto implica asegurarse de que los datos est√©n correctamente ordenados y que la columna de fecha y hora tenga el tipo de datos adecuado para facilitar el an√°lisis posterior. Adem√°s, es crucial identificar y tratar los valores faltantes, tanto de tiempo como de datos, ya que pueden sesgar los resultados si no se manejan adecuadamente. M√©todos como la interpolaci√≥n lineal, el relleno hacia adelante o hacia atr√°s, y la descomposici√≥n estacional y de tendencia pueden ser √∫tiles para imputar estos valores, garantizando as√≠ la integridad de la serie temporal (Gupta, 2022; Data Science Wizards, 2023).<br><b>Tratamiento de Datos Faltantes</b><br><ol>
  <li><b>Interpolaci√≥n</b>: Este m√©todo rellena los valores faltantes bas√°ndose en una funci√≥n matem√°tica que asume una transici√≥n suave entre los puntos conocidos adyacentes. Puede ser especialmente √∫til en series de tiempo con tendencias suaves, ya que preserva la continuidad y la tendencia de los datos. Sin embargo, en series con alta variabilidad, puede no capturar las fluctuaciones de manera precisa. Existen distintos m√©todos de llevar a cabo esta imputaci√≥n de datos faltantes.</li>
    <ul>
      <li><b>Lineal</b>: Es una t√©cnica com√∫nmente utilizada para imputar valores faltantes en series temporales. Consiste en trazar una l√≠nea recta entre los puntos adyacentes a los valores faltantes y estimar los datos intermedios bas√°ndose en esta l√≠nea. Este m√©todo asume que los cambios entre los puntos son constantes y uniformes, lo que lo convierte en una opci√≥n adecuada para datos que muestran una progresi√≥n suave y lineal. Es efectiva cuando la serie de tiempo sigue una tendencia clara o una relaci√≥n lineal entre observaciones. No obstante, si la serie presenta alta variabilidad o comportamientos no lineales, este m√©todo puede ser insuficiente, ya que no captura patrones complejos como estacionalidades o fluctuaciones repentinas (Hosseinabadi, 2022).</li>
      <li><b><i>Spline</b></i>: Este m√©todo es una t√©cnica avanzada de interpolaci√≥n que utiliza funciones spline, espec√≠ficamente splines c√∫bicos, para ajustar una serie de polinomios en tramos entre cada par de puntos de datos adyacentes. La interpolaci√≥n spline es particularmente √∫til para datos que presentan cambios suaves pero no necesariamente lineales, ya que garantiza que la curva resultante sea suave en las uniones entre tramos. Este m√©todo es ideal para situaciones donde la continuidad y la suavidad de la curva interpolada son importantes‚Äã (Crawstat, 2020).</li>
      <li><b>Polin√≥mica</b>: La interpolaci√≥n polin√≥mica consiste en ajustar un √∫nico polinomio de grado n-1 a trav√©s de todos los puntos de datos conocidos. A medida que el grado del polinomio aumenta, la curva resultante puede volverse m√°s precisa, pero tambi√©n puede ser propensa a oscilaciones y comportamientos no deseados, especialmente en los extremos de los datos. Este m√©todo es adecuado cuando los datos siguen una tendencia que puede ser bien representada por una funci√≥n polin√≥mica, aunque se debe tener cuidado con los problemas de sobreajuste‚Äã (Crawstat, 2020).</li>
    </ul>
  <li><b>Relleno hacia adelante y hacia atr√°s (<i>Forward / Backward Fill</i>)</b>: El m√©todo de relleno hacia adelante usa el √∫ltimo valor conocido antes de un dato faltante para llenar el vac√≠o, mientras que el m√©todo hacia atr√°s utiliza el siguiente valor disponible. Estos m√©todos son simples y efectivos en situaciones donde se asume que los valores no cambian dr√°sticamente en cortos periodos. Sin embargo, pueden introducir sesgos si los datos cambian significativamente (Crawstat, 2020; Hosseinabadi, 2022)‚Äã.</li>
  <li><b>Descomposici√≥n de tendencias y estacionalidad</b>: Este enfoque implica descomponer la serie de tiempo en componentes de tendencia y estacionalidad para imputar los valores faltantes en cada componente por separado. Este m√©todo es eficaz en series de tiempo donde los patrones estacionales o de tendencia son fuertes, permitiendo una imputaci√≥n m√°s precisa que respeta la estructura temporal subyacente (Crawstat, 2020).</li>
  <li><b><i>Exponential Smoothing</i> Aditivo</b>: El <i>Exponential Smoothing</i> Aditivo es un m√©todo de suavizado que se emplea principalmente en series temporales con tendencia. Este enfoque calcula un promedio ponderado de los datos pasados, donde los pesos disminuyen exponencialmente con el tiempo. En el contexto de imputaci√≥n de datos faltantes, este m√©todo es √∫til ya que permite mantener la tendencia en los datos mientras se interpolan los valores faltantes, proporcionando una estimaci√≥n m√°s precisa y coherente con la naturaleza del conjunto de datos. Este m√©todo es particularmente relevante en modelos de regresi√≥n donde es crucial preservar las caracter√≠sticas temporales de los datos para obtener predicciones confiables (Hyndman & Athanasopoulos, 2018; Taylor, 2003).</li>
  <li><b><i>Exponential Smoothing</i> Multiplicativo</b>: Por otro lado, el <i>Exponential Smoothing</i> Multiplicativo se utiliza cuando los datos presentan tanto tendencia como estacionalidad, y las variaciones estacionales multiplican los niveles de la serie temporal. Este m√©todo es clave para la imputaci√≥n de datos faltantes en situaciones donde la estacionalidad juega un papel importante en la variabilidad de los datos. Al aplicar un suavizado exponencial multiplicativo, se pueden rellenar los valores faltantes respetando tanto la tendencia como las variaciones estacionales, lo que es esencial para garantizar que los modelos de regresi√≥n mantengan su precisi√≥n al predecir valores futuros en presencia de estacionalidad (Hyndman & Athanasopoulos, 2018; Snyder et al., 2017).</li>
</ol><b>Descomposici√≥n de la Serie Temporal y Eliminaci√≥n de Ruido</b><br>Asimismo, la eliminaci√≥n de ruido en la serie temporal es un paso esencial para mejorar la precisi√≥n del an√°lisis. T√©cnicas como las medias m√≥viles y la Transformada de Fourier pueden ayudar a reducir las fluctuaciones no deseadas en los datos, proporcionando un conjunto de datos m√°s limpio y preparado para el modelado. Al seguir estos pasos de preprocesamiento, se puede asegurar que los datos est√©n listos para ser utilizados en modelos m√°s complejos, lo que permitir√° obtener resultados m√°s precisos y fiables (Aldean, 2023).<br>
<ul>
  <li><b>Media M√≥vil (<i>Rolling Mean</i> √≥ <i>Moving Average</i> (MA))</b>: La media m√≥vil es una t√©cnica utilizada para suavizar una serie temporal, donde se calcula el promedio de un conjunto de observaciones previas dentro de una ventana espec√≠fica. Esta media se computa de forma secuencial para cada ventana a lo largo de la serie, lo cual ayuda a reducir significativamente el ruido presente en los datos (Gupta, 2022).</li>
  <li><b>Transformada de Fourier</b>: La transformada de Fourier ofrece un m√©todo para eliminar el ruido al trasladar los datos de una serie temporal al dominio de la frecuencia. En este espacio, se pueden filtrar las frecuencias que contribuyen al ruido, y luego, mediante la transformada de Fourier inversa, se reconstruye la serie temporal, pero ahora con un nivel de ruido reducido (Gupta, 2022).</li>
</ul></h4>
<h3><b>Regresi√≥n Lineal M√∫ltiple</b></h3>
<h4>La regresi√≥n lineal m√∫ltiple es una t√©cnica estad√≠stica que permite modelar la relaci√≥n entre una variable dependiente y varias variables independientes. Este m√©todo es fundamental en el an√°lisis de datos porque permite evaluar el impacto conjunto de m√∫ltiples factores en una variable de inter√©s, lo que es crucial para hacer predicciones precisas y tomar decisiones informadas en proyectos de an√°lisis de datos. La ecuaci√≥n general de la regresi√≥n lineal m√∫ltiple se expresa como:<br>
<center>
<p style="text-align: center;">
    <h4><b><i>y</i> = <i>Œ≤<sub>0</sub> + Œ≤<sub>1</sub>x<sub>1</sub> + Œ≤<sub>2</sub>x<sub>2</sub> + ‚Ä¶ + Œ≤<sub>n</sub>x<sub>n</sub></i></b></h4>
</p>
</center>
donde <i>ùë¶</i> es la variable dependiente, <i>ùë•</i><sub>1</sub>, <i>ùë•</i><sub>2</sub>, ..., <i>ùë•</i><sub><i>n</i></sub> son las variables independientes, ùõΩ<sub>0</sub> es la intersecci√≥n y ùõΩ<sub>1</sub>, ùõΩ<sub>2</sub>, ..., ùõΩ<sub><i>n</i></sub> son los coeficientes asociados a cada variable independiente. La importancia de este modelo radica en su capacidad para capturar la influencia simult√°nea de m√∫ltiples variables, lo que es particularmente √∫til en an√°lisis econ√≥micos, cient√≠ficos y sociales. (Aggarwal, 2020; <i>Geeks for Geeks</i>, 2024).<br>En el contexto de <i>Python</i>, la biblioteca <i>Scikit-Learn</i> (o simplemente <i>sklearn</i>) facilita la implementaci√≥n de la regresi√≥n lineal m√∫ltiple a trav√©s de la clase <i>LinearRegression</i>, que se encuentra en el m√≥dulo <i>linear_model</i>. Esta herramienta permite ajustar el modelo a los datos, realizar predicciones y analizar la importancia de cada coeficiente en la ecuaci√≥n, haciendo que la regresi√≥n lineal m√∫ltiple sea accesible y eficiente para manejar grandes conjuntos de datos. La <i>LinearRegression</i> en <i>sklearn</i> tambi√©n incluye funciones para evaluar el rendimiento del modelo, lo que la convierte en una opci√≥n preferida para analistas de datos que buscan combinar simplicidad con potencia en sus an√°lisis (<i>sklearn</i>, s.f.).</h4>

<h3><b><i>K-Means</i></b></h3>
<h4>
El algoritmo <i>K-Means</i> es una herramienta poderosa para la identificaci√≥n y an√°lisis de conglomerados en conjuntos de datos grandes y complejos. Su simplicidad y efectividad lo han convertido en un est√°ndar en la miner√≠a de datos y el aprendizaje autom√°tico, permitiendo a los profesionales identificar patrones y relaciones que no ser√≠an evidentes a trav√©s de m√©todos m√°s tradicionales (Sanz, 2020).<br>En estad√≠stica un conglomerado (o cl√∫ster) es un grupo de objetos que son similares entre s√≠ y diferentes de objetos en otros grupos. El algoritmo <i>K-Means</i> busca identificar estos conglomerados dentro de un conjunto de datos, lo que es fundamental en muchos campos, como la segmentaci√≥n de mercado, la identificaci√≥n de patrones en im√°genes y la clasificaci√≥n de documentos (Sanz, 2020; Hartigan & Wong, 1979).<br><i>K-Means</i> es particularmente √∫til para identificar conglomerados cuando la estructura de los datos no es evidente a simple vista. Al dividir los datos en conglomerados, <i>K-Means</i> facilita la comprensi√≥n de las relaciones internas dentro de un conjunto de datos, permitiendo a los investigadores y analistas extraer conocimiento valioso de la informaci√≥n disponible (Hartigan & Wong, 1979).
</h4>

<h3><b>An√°lisis de Componentes Principales (PCA)</b></h3>
<h4>
El An√°lisis de Componentes Principales (PCA, por sus siglas en ingl√©s - <i>Principal Component Analysis</i>) es una t√©cnica de reducci√≥n de dimensionalidad que transforma un conjunto de variables posiblemente correlacionadas en un conjunto de valores de variables no correlacionadas llamadas componentes principales. Este m√©todo es particularmente √∫til cuando se trabaja con datos de alta dimensionalidad, ya que permite simplificar el modelo sin perder informaci√≥n cr√≠tica. PCA identifica las direcciones (componentes) en las que los datos var√≠an m√°s, permitiendo que los datos se proyecten en un espacio de menor dimensi√≥n, lo cual es crucial en tareas de clasificaci√≥n, regresi√≥n y visualizaci√≥n de datos complejos (Jolliffe, 2016; Shlens, 2014).<br>En Python, la biblioteca <i>sklearn</i> ofrece la implementaci√≥n del PCA a trav√©s de la clase PCA en el m√≥dulo <i>decomposition</i>. Esta herramienta permite extraer las componentes principales de un conjunto de datos, facilitando as√≠ la reducci√≥n de dimensionalidad y la visualizaci√≥n de datos en 2D o 3D. Adem√°s, <i>sklearn</i> proporciona m√©todos para determinar la cantidad de varianza explicada por cada componente, ayudando a seleccionar el n√∫mero adecuado de componentes a conservar. Esto es esencial para optimizar el rendimiento de los modelos y evitar el sobreajuste (Pedregosa et al., 2011; Raschka, 2015).
</h4>
<h3>En el contexto de un PCA, es esencial calcular el factor de inflaci√≥n de la varianza (VIF, por sus siglas en ingl√©s - <i>Variance Inflation Factor</i>) para identificar y reducir la multicolinealidad, que puede distorsionar los resultados y la interpretaci√≥n del an√°lisis.<br>
<b>Factor de inflaci√≥n de la varianza (VIF)</b></h3>
<h4>El Factor de Inflaci√≥n de la Varianza (VIF) es una medida que eval√∫a la multicolinealidad entre las variables independientes en un modelo de regresi√≥n. Si los valores de VIF son elevados, significa que existe una fuerte correlaci√≥n entre las variables, lo que podr√≠a interferir con la efectividad del PCA al intentar reducir la dimensionalidad del conjunto de datos. Por lo tanto, reducir la multicolinealidad mediante la revisi√≥n de los VIF es un paso crucial para garantizar que el PCA funcione correctamente, proporcionando componentes principales que capturen mejor la varianza en los datos originales (Chatterjee & Hadi, 2015). De acuerdo con Hern√°ndez, Usuga y Mazo (2024), los siguientes son los valores a considerar en la obtenci√≥n del VIF:<br>
<ul>
<li>Si VIF ‚â§ 5, entonces no hay problemas de multicolinealidad.</li>
<li>Si 5 < VIF ‚â§ 10, entonces hay problemas de multicolinealidad moderada.</li>
<li>Si VIF > 10, entonces hay problemas de multicolinealidad graves.</li>
</ul>
</h4>
"""

'''
Librer√≠as para el tratamiento y visualizaci√≥n b√°sicos de los datos, as√≠ como para evitar mensajes innecesarios.
'''
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings("ignore")

'''
Librer√≠as para la imputaci√≥n de datos faltantes, segmentaci√≥n, estandarizaci√≥n y an√°lisis de componentes principales
'''
from statsmodels.tsa.holtwinters import ExponentialSmoothing as ES
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

'''
Para evitar multicolinealidad con el PCA
'''
from statsmodels.stats.outliers_influence import variance_inflation_factor
from patsy import dmatrices

'''
Librer√≠as y m√©todos para la regresi√≥n lineal y evaluaci√≥n
'''
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import pearsonr
import statsmodels.api as sm

'''
Librer√≠as para obtener estad√≠sticos
'''
import scipy.stats as ss
import statsmodels.api as sm

'''
Se importan los componentes necesarios para el an√°lisis de conglomerados, incluyendo su evaluaci√≥n para la diferenciaci√≥n
'''
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Lectura del archivo
df = pd.read_csv('Obispado_allmonths (3).csv')
df.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)
df

# Informaci√≥n del dataset
df.info()

# Estad√≠stica descriptiva del dataset
df.describe().T

# Observaci√≥n de los datos faltantes
print(f"Cantidad de datos faltantes: {df.isnull().sum().sum()}")
print(f"Cantidad de datos faltantes por columna:\n{df.isnull().sum()}")

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3>
El dataset descrito consta de 120 entradas con 12 columnas, de las cuales una es de tipo <i>object (Date)</i> y las otras 11 son de tipo <i>float64</i>. Se observa que la columna <i>PM2.5</i> tiene 9 valores faltantes. Se experimentar√°n diferentes m√©todos de imputaci√≥n de datos faltantes, como interpolado lineal, <i>spline</i> de orden 2 y 3, <i>polynomial</i> de orden 2 y 3, <i>forward</i> y <i>backward filling</i>, media m√≥vil, y suavizamiento exponencial aditivo y multiplicativo, as√≠ como una regresi√≥n preliminar (PR) para datos no faltantes, para posteriormente emplear estos datos en regresi√≥n lineal, PCA y K-Means. Se observar√°n sus dimensiones, analizar√°n sus tendencias en los periodos faltantes en comparaci√≥n con el hist√≥rico de datos y, en funci√≥n de sus errores, se seleccionar√° el mejor m√©todo de imputaci√≥n de datos. Para que tener la certeza de que el modelo da resultados confiables, se realizar√° una prueba global del mismo y, con ello, poder rechazar o aceptar con evidencia los datos imputados.
</h3>
"""

# Regresi√≥n preliminar para sustituir datos faltantes
df_train = df[df['PM2.5'].isna()]
df_test = df[df['PM2.5']>0]
X_train = df.drop(['Date', 'PM2.5'], axis=1)
Y = df['PM2.5']

df_test_X = df_test.drop(['Date', 'PM2.5'], axis=1)
df_test_Y = df_test['PM2.5']

model = LinearRegression()

# Se ajusta el modelo con datos de entrenamiento
model.fit(df_test_X, df_test_Y)

# Se realizan predicciones con los datos de prueba
y_pred = model.predict(df_test_X)

# Calculamos el MSE con datos no faltantes
mse_test = mean_squared_error(df_test_Y, y_pred)

# Predecimos valores faltantes con la regresi√≥n
df_train_X = df_train.drop(['Date', 'PM2.5'], axis=1)
predicted_pm25 = model.predict(df_train_X)

# Reemplazamos los valores faltantes con los de la nueva predicci√≥n
df_train['PM2.5'] = predicted_pm25

# Concatenamos los datos de entrenamiento y prueba
df_combined = pd.concat([df_train, df_test], axis=0)

# Ordenamos los datos por fecha, como
df_combined.sort_values(by='Date', inplace=True)

# Colocamos los datos de la predicci√≥n en una nueva columna: Regresi√≥n Preliminar (PR)
df['PM2.5_PR'] = df_combined['PM2.5']

# Interpolaci√≥n, FFill y BFill
for method in ['linear', 'spline', 'polynomial', 'ffill', 'bfill']:
  if method == 'linear':
    df['PM2.5_IL'] = df['PM2.5'].interpolate(method=method) # M√©todo IL
  else:
    # Interpolados spline y polinomial de orden 2 y 3
    if method == 'spline':
      for order in range(2, 4):
        df['PM2.5_SP'+str(order)] = df['PM2.5'].interpolate(method=method, order=order)
    if method == 'polynomial':
      for order in range(2, 4):
        df['PM2.5_PL'+str(order)] = df['PM2.5'].interpolate(method=method, order=order)
    # M√©todos ffill y bfill
    if method == 'ffill':
      df['PM2.5_FF'] = df['PM2.5'].ffill()
    if method == 'bfill':
      df['PM2.5_BF'] = df['PM2.5'].bfill()

# Imputaci√≥n con Media M√≥vil
df['PM2.5 Media M√≥vil'] = df['PM2.5'].fillna(df['PM2.5'].rolling(window = 12, min_periods = 1).mean())

# Suavizamiento Exponencial Aditivo
model = ES(df['PM2.5'].dropna(), trend='add', seasonal='add', seasonal_periods=12)
fit = model.fit()
df['PM2.5_ES+'] = df['PM2.5']
df.loc[df['PM2.5_ES+'].isna(), 'PM2.5_ES+'] = fit.predict(start=0, end=len(df)-1)[df['PM2.5_ES+'].isna()]

# Suavizamiento Exponencial Multiplicativo
model = ES(df['PM2.5'].dropna(), trend='mul', seasonal='mul', seasonal_periods=12)
fit = model.fit()
df['PM2.5_ES*'] = df['PM2.5']
df.loc[df['PM2.5_ES*'].isna(), 'PM2.5_ES*'] = fit.predict(start=0, end=len(df)-1)[df['PM2.5_ES*'].isna()]

# Observaci√≥n
df

# Estad√≠sticos descriptivos de nuevas columnas
df[['PM2.5'] + [col for col in df.columns if 'PM2.5' in col and not df[col].isnull().any()]].describe().T

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b><i>Insights</i></b></h3>
<h4>
Los nuevos datos candidatos para la imputaci√≥n cuentan con las m√©tricas anteriores. Se aprecia, en la tabla de estad√≠sticos descriptivos, que PM2.5 con Spline de orden 2 y 3 cuenta con dimensiones, incluyendo desviaci√≥n est√°ndar y promedio (para Spline de orden 3 en particular) totalmente alejados del resto de modelos, incluso negativos. Se visualizar√°n estos valores con <i>boxplots</i> y, de mantenerse aislados, se descartar√°n para la imputaci√≥n final. Adem√°s, en el nuevo <i>dataset</i> se observa que los interpolados con m√©todo polinomial de orden 2 y 3, as√≠ como con <i>backward fill</i> cuentan a√∫n con datos nulos, es decir, aquello que se desea sustituir, debido a la naturaleza de sus algoritmos. Por ello, quedan nuevamente descartados.
</h4>
"""

cols = df[['PM2.5'] + [col for col in df.columns if 'PM2.5' in col and not df[col].isnull().any()]]
fig = make_subplots(rows = 1, cols = len(cols.columns))

for i, col in enumerate(cols.columns):
  fig.add_trace(go.Box(y = df[col], name = col), row=1, col = i+1)

fig.update_layout(height=600, width=1350, title_x = 0.5, title_text="Boxplots de las variables", template='plotly_dark')
fig.show()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">

<h3><b><i>Insights</i></b></h3>
<h4>
Los resultados de la interpolaci√≥n usando <i>Spline</i>, tanto para orden 2 como para orden 3, han arrojado una cantidad inadmisible de <i>outliers</i> o valores at√≠picos. Por ello, se descartar√°n y se tomar√°n en cuenta √∫nicamente los otros modelos con tendencias y periodos de estacionalidad significativos que se visualizar√°n obteniendo el cambio en la cantidad de part√≠culas PM 2.5 con respecto a periodos anteriores que mantengan un comportamiento lo m√°s parecido posible al de los registros originales de part√≠culas PM 2.5 sin imputaci√≥n. Cabe destacar que se descartan igualmente m√©todos que siguen contando con valores faltantes, como el <i>backward fill</i> dada su naturaleza. Posteriormente, se visualizar√° el Error Cuadr√°tico Medio (MSE, por sus siglas en ingl√©s - <i>Mean Squared Error</i>) y, con base en ello, ser√°n imputados los resultados de dicho modelo en aquellos valores faltantes de la columna PM 2.5 original. A continuaci√≥n, se obtiene y visualiza el cambio con respecto a periodos anteriores de los m√©todos de imputaci√≥n restantes, rechazando aquellos que se mantengan sin variaci√≥n en los periodos faltantes al no coincidir con la tendencia y estacionalidad de la serie original de PM 2.5
</h4>
"""

new_cols = []
for col in cols.columns:
  if 'SP3' not in col and 'SP2' not in col:
    df['Change_'+col] = df[col].div(df[col].shift())
    new_cols += ['Change_'+col]
pd.set_option('display.float_format', lambda x: '%.4f' % x)
df.head(20)

fig = px.line(df.loc[df['Date']>'2012'], x='Date', y=new_cols, template='plotly_dark',
              labels={'value': 'Concentraci√≥n de PM2.5', 'Date': 'Fecha'}, title='<b>Cambio en la cantidad de part√≠culas PM2.5 en MTY por Imputaci√≥n</b>')

# Personalizaci√≥n de las l√≠neas
fig.update_traces(selector=dict(name='Change_PM2.5'), line=dict(color='lightgreen', dash='solid', width=4), name='PM2.5 Original')

colors = ['red', 'blue', 'purple', 'magenta', 'lightblue', 'white']
names = ['PM2.5 - Regresi√≥n Preliminar', 'PM2.5 Interpolado Lineal', 'PM2.5 Forward Filling', 'PM2.5 - Media M√≥vil', 'PM2.5 ES Aditivo',
         'PM2.5 ES Multiplicativo']
dashes = ['dash', 'dot', 'dashdot', 'longdash', 'longdashdot', 'dash']

for col, color, name, dash in zip(new_cols[1:], colors, names, dashes):
  fig.update_traces(selector=dict(name=col), line=dict(color=color, dash=dash), name=name)

# Ajuste de la leyenda
fig.update_layout(legend_title_text='Cambio en Series de PM2.5', title_x=0.5)

# Mostrar la figura
fig.show()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">

<h3><b><i>Insights</i></b></h3>
<h4>
En este gr√°fico utilizamos la linea verde que muestra la serie original como una referencia para evaluar la precisi√≥n de cada m√©todo de imputaci√≥n. Podemos observar que la interpolaci√≥n lineal y la media m√≥vil son las que mas se acercan a esa serie original. Comparten las mismas tendencias y cambios en la grafica.
El m√©todo de regresi√≥n preliminar muestra mucha variabilidad y se desalinea mucho de la original en ciertos puntos, principalmente de enero a agosto de 2013 y agosto a diciembre de 2015. El m√©todo de forward filling igual se ve que sigue la tendencia y los cambios de la serie original, aunque tiene unas inconsistencias que podr√≠an afectar a nuestro modelo predictivo.
M√©todos como el de media m√≥vil pueden parecer muy bien pero al suavizar demasiado pueden ocultar ciertos patrones cr√≠ticos, mientras que la interpolaci√≥n lineal alcanza un buen equilibro entre la precisi√≥n y la suavidad. Depende de lo que se busque en el problema se puede elegir un diferente modelo, si se busca un an√°lisis mas general, opciones que capturen la variabilidad o que suavicen mas, podr√≠an ser una buena opci√≥n. Mientras que otras opciones como serian la interpolaci√≥n lineal y el forward filling serian opciones mas equilibradas para mantener la misma estructura de los datos originales.
</h4>
"""

columns = ['PM2.5'] + [col for col in df.columns if 'SP3' not in col and 'SP2' not in col and not df[col].isnull().any()]
df = df[columns]

fig = px.line(df, x='Date', y=['PM2.5', 'PM2.5_IL', 'PM2.5_FF', 'PM2.5 Media M√≥vil', 'PM2.5_ES+', 'PM2.5_ES*', 'PM2.5_PR'], template='plotly_dark',
              labels={'value': 'Concentraci√≥n de PM2.5', 'Date': 'Fecha'},
              title='<b>Visualizaci√≥n de Imputaci√≥n de Datos de Part√≠culas PM 2.5 en MTY con M√©todos Variados (2006 - 2013)</b>')
y_cols = ['PM2.5_IL', 'PM2.5_FF', 'PM2.5 Media M√≥vil', 'PM2.5_ES+', 'PM2.5_ES*', 'PM2.5_PR']

# Personalizaci√≥n de las l√≠neas
fig.update_traces(line=dict(width=2), opacity=0.8)
fig.update_traces(selector=dict(name='PM2.5'), line=dict(color='white', width=3), name='PM2.5 Original')

colors = ['blue', 'magenta', 'red', 'cyan', 'yellow', 'lightblue']
dashes = ['dash', 'dot', 'dashdot', 'longdash', 'longdashdot', 'dot']
names = ['PM2.5 Interpolado Lineal', 'PM2.5 Forward Filling', 'PM2.5 - Media M√≥vil', 'PM2.5 ES Aditivo', 'PM2.5 ES Multiplicativo',
         'PM2.5 Regresi√≥n Preliminar']

for col, color, dash, name in zip([c for c in y_cols if 'PM2.5' in c and c != 'PM2.5' and 'Change' not in c], colors, dashes, names):
  fig.update_traces(selector=dict(name=col), line=dict(color=color, dash=dash), name=name)

# Ajuste de la leyenda
fig.update_layout(legend_title_text='Series de PM2.5', title_x=0.5)

# Mostrar la figura
fig.show()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">

<h3><b><i>Insights</i></b></h3>
<h4>
Se visualizan los m√©todos de imputaci√≥n restantes y se comparan con la serie original. Por medio de la gr√°fica se aprecia el nulo cambio en los periodos faltantes de PM 2.5 con el m√©todo de <i>forward filling</i> dada su naturaleza, mientras que, por la misma raz√≥n junto con la irregularidad en los intervalos de la serie original, el interpolado lineal luce poco cre√≠ble, en especial para un periodo de 6 meses, por lo que queda descartado y se consideran √∫nicamente los m√©todos de predicci√≥n con media m√≥vil, suavizamiento exponencial aditivo y multiplicativo y la regresi√≥n preliminar para datos faltantes.
</h4>
"""

# Datos de los MSEs para modelos a usar
mse_results = {}

# Obtenci√≥n de valores del ES aditivo con MSE
es_model = ES(df['PM2.5'].dropna(), trend='add', seasonal='add', seasonal_periods=12).fit()
es_pred = es_model.predict(start=0, end=len(df['PM2.5'].dropna()) - 1)
mse_results['ES Aditivo'] = mean_squared_error(
    df['PM2.5'].dropna(),  # Datos originales no NaN
    es_pred  # Predicciones ES+ para datos originales
)

# Obtenci√≥n de valores del ES multiplicativo con MSE
es_model = ES(df['PM2.5'].dropna(), trend='mul', seasonal='mul', seasonal_periods=12).fit()
es_pred = es_model.predict(start=0, end=len(df['PM2.5'].dropna()) - 1)
mse_results['ES Multiplicativo'] = mean_squared_error(
    df['PM2.5'].dropna(),  # Datos originales no NaN
    es_pred  # Predicciones ES* para datos originales
)

# Obtenci√≥n de valores de predicci√≥n con MA con MSE
mse_results['Media M√≥vil'] = mean_squared_error(
    df['PM2.5'].dropna(), # Datos originales no NaN
    df['PM2.5'].dropna().rolling(window = 12, min_periods = 1).mean()  # Predicciones MA para datos originales
)

# Visualizaci√≥n con un dataframe ordenado, junto con su RMSE
mse_results['Regresi√≥n Lineal'] = mse_test # MSE de la regresi√≥n preliminar
mse_df = pd.DataFrame(list(mse_results.items()), columns=['M√©todo', 'MSE'])
mse_df['RMSE'] = np.sqrt(mse_df['MSE'])
mse_df.sort_values(by='MSE')

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">

<h3><b><i>Insights</i></b></h3>
<h4>
El modelo con menor MSE para la imputaci√≥n de datos es la regresi√≥n preliminar. Es ideal usarlo para la imputaci√≥n de datos. Se conservan los datos no faltantes originales, mientras que los datos faltantes ser√°n reemplazados por las predicciones del modelo de regresi√≥n preliminar. Se realizar√° una prueba de hip√≥tesis de la significancia del modelo para asegurarnos de que sus resultados son confiables. De rechazarse la hip√≥tesis, se mantendr√°n los resultados obtenidos con la imputaci√≥n del suavizamiento exponencial multiplicativo por ser el segundo m√©todo de imputaci√≥n con menor error aparte de la regresi√≥n preliminar.</h4>
"""

# Observaciones y predicciones
df = df_combined.copy()
n = len(df_test_Y)

k = df_test_X.shape[1]  # Variables independientes
u = k
v = n - (k + 1)

# Modelo
model = LinearRegression()
model.fit(df_test_X, df_test_Y)

# Predicciones y residuos
y_pred = model.predict(df_test_X)

# Predicciones y residuos
residuals = df_test_Y - y_pred

# Suma de cuadrados
SSR = np.sum((y_pred - np.mean(df_test_Y))**2)
SSE = np.sum(residuals**2)
SST = SSR + SSE

# Errores promedio
MSR = SSR / k
MSE = SSE / v

# R2
R2 = model.score(df_test_X, df_test_Y)

# F-Stat
F_cal = MSR / MSE

a = 0.05

# Valor cr√≠tico de F
F_critical = ss.f.ppf(1 - a, u, v)

# p-valor de F-Stat
p_value_F = 1 - ss.f.cdf(F_cal, u, v)

# Prueba Global del Modelo de Regresi√≥n Lineal M√∫ltiple para la Imputaci√≥n de NaNs en PM2.5
print(f"R^2 del modelo de regresi√≥n lineal m√∫ltiple: {R2:.4f}")
print(f"Estad√≠stico F asociado al modelo: {F_cal:.4f}")
print(f"Valor cr√≠tico de F (significancia = 0.05): {F_critical:.4f}")
print(f"p-value de F: {p_value_F:.4f}\n")
# Resultado de Prueba de hip√≥tesis
if p_value_F < a:
  print('Se rechaza H0. El modelo es estad√≠sticamente significativo.')
else:
  print('No se rechaza H0. El modelo no es estad√≠sticamente significativo.')

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b><i>Insights</i></b></h3>
<h4>
Tras ser elegido el modelo preliminar de regresi√≥n m√∫ltiple para datos faltantes, se observa que se rechaza nuestra hip√≥tesis nula, es decir, que el modelo no es significativo. Por lo anterior, no hay evidencia para rechazar los datos que ha arrojado que, de acuerdo con nuestros estad√≠sticos calculados tanto para el F asociado al modelo como para su p valor, los resultados del modelo son cre√≠bles. A continuaci√≥n se realizar√°, inicialmente, la regresi√≥n m√∫ltiple para los datos tanto con PCA como sin PCA. Para este √∫ltimo, se tomar√° en cuenta una posible multicolinealidad visualizada, o no, en una matriz de correlaci√≥n entre los contaminantes y las part√≠culas PM 2.5 y, de presentarse alguna, o algunas, que den problemas, ser√°n reducidas mediante PCA, ya que la p√©rdida de informaci√≥n no ser√° significativa y, de ser as√≠, ser√° comparado con el modelo sin PCA para elegir uno final.</h4>
<h2><b>An√°lisis de Datos</b></h2>
<h3><b>Implementaci√≥n del Modelo de Regresi√≥n Lineal M√∫ltiple</b></h3>
<h4><b>Modelo Sin An√°lisis de Componentes Principales (PCA)</b></h4>
"""

# Visualizaci√≥n de la Serie sin Datos Faltantes
fig = px.line(df, x='Date', y='PM2.5', title='<b>Serie de Tiempo de Part√≠culas PM 2.5 en MTY (2006 - 2013)</b>',
              color_discrete_sequence=['yellow'], template='plotly_dark', labels={'PM2.5': 'Concentraci√≥n de PM 2.5', 'Date': 'Fecha'})
# Ajuste de la leyenda
fig.update_layout(title_x=0.5)
fig.show()

fig = make_subplots(rows = 1, cols = len(df.columns[1 : ]))

for i, col in enumerate(df.columns[1:]):
  fig.add_trace(go.Box(y = df[col], name = col), row=1, col = i+1)

fig.update_layout(height=600, width=1200, title_x = 0.5, title_text="<b>Boxplots de las variables</b>", template='plotly_dark')
fig.show()

# Inspecci√≥n visual linealidad
fig = make_subplots(rows=6, cols=2, subplot_titles=df.columns[1:])

for i, col in enumerate(df.columns[1:]):
    row = (i // 2) + 1
    col_plot = (i % 2) + 1

    X = sm.add_constant(df[col])
    model = sm.OLS(df['PM2.5'], X).fit()
    predictions = model.predict(X)

    fig.add_trace(go.Scatter(x=df[col], y=df['PM2.5'], mode='markers', name=col), row=row, col=col_plot)
    fig.add_trace(go.Scatter(x=df[col], y=predictions, mode='lines', name=col), row=row, col=col_plot)

fig.update_layout(height=1200, width=1300, title_text="<b>Variables independientes vs PM2.5</b>", template='plotly_dark')
fig.show()

corr = df.drop(['Date'], axis=1).corr()

# Matriz sin repeticiones (tri√°ngulo inferior)
df_lt = corr.where(np.tril(np.ones(corr.shape)).astype(bool))

# Convertir la matriz a formato de texto, redondear y reemplazar ceros con cadenas vac√≠as
df_lt_text = df_lt.applymap(lambda x: f'{x:.4f}' if pd.notna(x) and x != 0 else '')

# Crear la gr√°fica de calor
fig = px.imshow(df_lt, text_auto=False, aspect="auto", color_continuous_scale='haline_r', zmin=-1, zmax=1,
                template='plotly_dark', height=600, width=1200)

# A√±adir un trazo alrededor de cada celda
fig.update_traces(hovertemplate=None, hoverinfo='skip', selector=dict(type='heatmap'), xgap=1, ygap=1, colorbar=dict(title='Value'))

# A√±adir texto personalizado a las celdas
fig.update_traces(text=df_lt_text.values, texttemplate='%{text}', textfont=dict(size=12))

# Configurar el t√≠tulo y las etiquetas
fig.update_layout(title='<b>Matriz de Correlaci√≥n de las Variables para la Regresi√≥n</b>', title_x=0.5, xaxis_title='Variable', yaxis_title='Variable')

# Mostrar la gr√°fica
fig.show()

# Pruebas de hip√≥tesis de los coeficientes de correlaci√≥n con PM 2.5
from scipy.stats import pearsonr

for col in df.drop(['Date'], axis=1).corr().columns:
  corr_coef, p_value = pearsonr(df['PM2.5'], df[col])
  print(f"Correlaci√≥n entre PM2.5 y {col}:")
  print(f"Coeficiente: {corr_coef}")
  print(f"P-valor: {p_value}")
  if p_value < 0.05:
    print("La correlaci√≥n es estad√≠sticamente significativa. ‚úîÔ∏è‚Äã‚Äã\n")
  else:
    print("La correlaci√≥n NO es estad√≠sticamente significativa. ‚Äã‚ùå‚Äã‚Äã\n")

# Separamos las variables independientes y
X = df.drop(['Date', 'PM2.5'], axis=1)
y = df['PM2.5']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)

model = LinearRegression().fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)
r2_adj_train = 1 - (1 - r2_train) * (len(y_train) - 1) / (len(y_train) - X_train.shape[1] - 1)
r2_adj_test = 1 - (1 - r2_test) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

print(f"R^2 ajustado para el entrenamiento: {r2_adj_train:.4f}")
print(f"R^2 ajustado para la prueba: {r2_adj_test:.4f}")

fig = go.Figure()
fig.add_trace(go.Scatter(x = df.index, y=df['PM2.5'], mode='lines', name='PM2.5', line=dict(color='white')))
fig.add_trace(go.Scatter(x = X_train.index, y=y_train_pred, mode='lines', name='Predicci√≥n PM2.5 (Train)', line=dict(color='red')))
fig.add_trace(go.Scatter(x = X_test.index, y=y_test_pred, mode='lines', name='Predicci√≥n PM2.5 (Test)', line=dict(color='blue')))

fig.update_layout(title='<b>Regresi√≥n Lineal M√∫ltiple para PM2.5</b>', xaxis_title='Fecha', yaxis_title='PM2.5', template='plotly_dark', title_x=0.5,
                  legend_title_text='Datos vs. Modelo')
fig.show()

y_pred_combined = np.concatenate([y_train_pred, y_test_pred])
y_original = np.concatenate([y_train, y_test])

n = len(y_pred_combined)

k = X_test.shape[1]  # Variables independientes
u = k
v = n - (k + 1)

# Predicciones y residuos
residuals = y_original - y_pred_combined

# Suma de cuadrados
SSR = np.sum((y_pred_combined - np.mean(y_original))**2)
SSE = np.sum(residuals**2)
SST = SSR + SSE

# Errores promedio
MSR = SSR / k
MSE = SSE / v

# F-Stat
F_cal = MSR / MSE

a = 0.05

# Valor cr√≠tico de F
F_critical = ss.f.ppf(1 - a, u, v)

# p-valor de F-Stat
p_value_F = 1 - ss.f.cdf(F_cal, u, v)

# Prueba Global del Modelo de Regresi√≥n Lineal M√∫ltiple para la Imputaci√≥n de NaNs en PM2.5
print(f"R^2 del modelo de regresi√≥n lineal m√∫ltiple: {r2_score(y_original, y_pred_combined):.4f}")

print(f"Estad√≠stico F asociado al modelo: {F_cal:.4f}")

print(f"Valor cr√≠tico de F (significancia = 0.05): {F_critical:.4f}\n")

print(f"p-value de F (Entrenamiento): {p_value_F:.4f}")

# Resultado de Prueba de hip√≥tesis
if p_value_F < a:
  print('Se rechaza H0. El modelo es estad√≠sticamente significativo.')
else:
  print('No se rechaza H0. El modelo es estad√≠sticamente significativo.')

from sklearn.model_selection import TimeSeriesSplit

# Definimos el n√∫mero de divisiones
tscv = TimeSeriesSplit(n_splits=5)

scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')
scores_adj = 1 - (1 - scores) * (len(y) - 1) / (len(y) - X.shape[1] - 1)
print("Valores de R^2 ajustado de la validaci√≥n cruzada:", scores_adj)
print("Promedio de los valores de R^2 ajsutado:", scores_adj.mean())

# Calcular la varianza de los residuos
residual_sum_of_squares = np.sum(residuals**2)
degrees_of_freedom = len(y) - len(model.coef_) - 1
residual_variance = residual_sum_of_squares / degrees_of_freedom

# Calcular la varianza de los coeficientes
X_with_const = np.c_[np.ones(X.shape[0]), X]  # Agregar una columna de unos para el intercepto
cov_matrix = np.linalg.inv(X_with_const.T @ X_with_const) * residual_variance
standard_errors = np.sqrt(np.diag(cov_matrix))

# Calcular los valores t y los valores p
t_values = model.coef_ / standard_errors[1:]  # Excluir el error est√°ndar del intercepto
p_values = [2 * (1 - ss.t.cdf(np.abs(t), degrees_of_freedom)) for t in t_values]

coef_df = pd.DataFrame({'Variable': X.columns, 'Coeficiente': model.coef_,
                        'Error Est√°ndar': standard_errors[1:], 'Valor t': t_values, 'Valor p': p_values})

intercept_df = pd.DataFrame({'Variable': ['Intersecci√≥n'], 'Coeficiente': [model.intercept_],
                             'Error Est√°ndar': [standard_errors[0]], 'Valor t': [model.intercept_ / standard_errors[0]],
                             'Valor p': [2 * (1 - ss.t.cdf(np.abs(model.intercept_ / standard_errors[0]), degrees_of_freedom))]})

summary_df = pd.concat([coef_df, intercept_df], ignore_index=True)
summary_df['Significativo'] = summary_df['Valor p'] < 0.05
summary_df

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Resultados</b></h3>
<h4>
El primer modelo de regresi√≥n lineal m√∫ltiple aplicado para predecir los niveles de PM2.5 mostr√≥ un desempe√±o decente en el conjunto de entrenamiento, con un R<sup>2</sup> ajustado de 0.6979, lo que sugiere que el modelo es capaz de explicar una porci√≥n considerable de la variabilidad en los datos de entrenamiento. Sin embargo, en el conjunto de prueba, el R<sup>2</sup> ajustado fue negativo (-0.1155), indicando que el modelo no generaliz√≥ bien, y que podr√≠a estar sobreajustado a los datos de entrenamiento. Adem√°s, la validaci√≥n cruzada utilizando <i>TimeSeriesSplit</i> con 5 divisiones revel√≥ una variabilidad considerable en los valores de R<sup>2</sup> ajustado entre las particiones, con un promedio de -0.2694, lo que confirma que el modelo no se desempe√±√≥ consistentemente a trav√©s de los diferentes subconjuntos temporales. Esta variabilidad resalta la importancia de la validaci√≥n cruzada en series temporales, ya que permite evaluar c√≥mo el modelo podr√≠a comportarse en futuros datos no vistos y c√≥mo la selecci√≥n del n√∫mero de divisiones puede influir en los resultados finales. A pesar de lo anterior, la prueba de hip√≥tesis realizada indica que el modelo es estad√≠sticamente significativo y, a√∫n as√≠, sus resultados pueden ser confiables.</h4>
<h3><b>Implementaci√≥n del An√°lisis de Conglomerados con <i>K-Means</i></b></h3>
<h4><b>Modelo Sin An√°lisis de Componentes Principales (PCA)</b></h4>
"""

# Seleccionamos columnas num√©ricas
X = df.drop(columns=['Date'])

# Estandarizaci√≥n
X_scaled = StandardScaler().fit_transform(X)

# Rango de K
range_n_clusters = range(2, 11)

# Coeficientes de silueta
silhouette_scores = []
for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Se organizan los resultados del coeficiente
silhouette_df = pd.DataFrame({'n_clusters': range_n_clusters, 'silhouette_score': silhouette_scores})

# Se grafican los coeficientes
fig = px.line(silhouette_df, x='n_clusters', y='silhouette_score',
              title='<b>Coeficiente de la Silueta para Distintos Clusters</b>',
              labels={'n_clusters': 'Number of Clusters', 'silhouette_score': 'Silhouette Score'}, template='plotly_dark', color_discrete_sequence=['blue'])
fig.update_layout(title_x=0.5)
fig.show()

# Aplicamos KMeans con k = 2
kmeans = KMeans(n_clusters=2, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# Add cluster labels to the DataFrame
df['cluster'] = cluster_labels

# Reducir la dimensionalidad a 3 componentes principales usando PCA
pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_scaled)

# Crear un DataFrame con los componentes principales y las etiquetas de cluster
df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2', 'PC3'])
df_pca['cluster'] = cluster_labels

# Graficamos los datos
fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PC3',
                    color='cluster', title='<b>Agrupamiento para K = 2 Con PCA para Visualizaci√≥n</b>', template='plotly_dark', width=800)
fig.update_layout(title_x=0.5)
fig.show()

# Inspecci√≥n visual linealidad
fig = make_subplots(rows=6, cols=2, subplot_titles=df.columns[1:])
cols = [col for col in df.columns if col not in ['cluster', 'Date']]

# Colores por cl√∫ster
cluster_colors = {0: 'blue', 1: 'yellow'}

for i, col in enumerate(df.columns[1:len(df.columns)-1]):
  row = (i // 2) + 1
  col_plot = (i % 2) + 1

  fig.add_trace(go.Scatter(x=df[col], y=df['PM2.5'], mode='markers', marker_color=df['cluster'], showlegend=False), row=row, col=col_plot)

fig.update_layout(height=1000, width=1300, title_text="<b>Predictores vs PM2.5 por Cluster</b>", title_x=0.5, template='plotly_dark')
fig.show()

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
num_cols = list(df.select_dtypes(include=numerics).columns)
df.groupby('cluster')[num_cols].mean()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Resultados</b></h3>
<h4>
En el an√°lisis de <i>K-Means</i> con K = 2 y sin aplicar PCA, los resultados revelaron dos grupos distintos, con un coeficiente de silueta de 0.29, lo que indica una separaci√≥n moderadamente d√©bil entre los cl√∫steres. Al observar las variables involucradas, uno de los grupos podr√≠a estar asociado con niveles elevados de contaminantes como NO2, NOX, y PM10, lo cual sugiere zonas o periodos de alta contaminaci√≥n, mientras que el otro grupo muestra niveles relativamente m√°s bajos, sugiriendo condiciones de menor contaminaci√≥n. Estos grupos podr√≠an etiquetarse como "Alta Contaminaci√≥n" y "Baja Contaminaci√≥n".</h4>
<h3><b>Implementaci√≥n de Modelo de Regresi√≥n Lineal M√∫ltiple</b></h3>
<h4><b>Modelo Con An√°lisis de Componentes Principales (PCA)</b></h4>
"""

# C√°lculo del factor de inflaci√≥n de la varianza. Renombrando una columna
df_temp = df.rename(columns={'PM2.5': 'PM2_5'})

formula = 'PM2_5 ~ CO + NO + NO2 + NOX + O3 + PM10 + PRS + RH + SR + TOUT'
y, X = dmatrices(formula, df_temp, return_type='dataframe')

vif = pd.DataFrame()
vif['Variable'] = X.columns
vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

# Revertir el nombre de la columna
df_temp = df_temp.rename(columns={'PM2_5': 'PM2.5'})
vif.sort_values(by='VIF', ascending=False)

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b><i>Insights</i></b></h3>
<h4>
Con VIFs extremadamente altos, las variables NOX, NO2, y NO est√°n altamente correlacionadas entre s√≠, lo que podr√≠a provocar problemas para los modelos anteriores. En su lugar, y como se coment√≥ en la secci√≥n de metodolog√≠a, se opta por eliminar NOX y NO2 para reducir la redundancia. Este paso es importante para evitar que las variables altamente correlacionadas dominen los primeros componentes principales en el PCA.<br>A continuaci√≥n, es necesario estandarizar las variables restantes (es decir, escalarlas para que tengan media 0 y desviaci√≥n est√°ndar 1), como se hizo en la implementaci√≥n de <i>K-Means</i> sin PCA, porque el PCA es sensible a las diferencias en escala.</h4>
"""

# Dataset reducido
# df_reduced = df.drop(columns=['NOX', 'NO2'])

# Se separan las variables independientes de la variable objetivo (PM2.5) para la regresi√≥n (si es KMeans, no se descarta PM 2.5)
X = df.drop(columns=['Date', 'PM2.5'])

# Se estandarizan las variables independientes
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X[['NO', 'NO2', 'NOX']])

# Se aplica PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Varianza explicada acumulada
var_exp_cumulative = pca.explained_variance_ratio_.cumsum()

# Visualizaci√≥n
fig = px.line(x=range(1, len(var_exp_cumulative)+1), y=var_exp_cumulative, markers=True,
              labels={'x': 'N√∫mero de Componentes', 'y': 'Varianza Explicada Acumulada'},
              title='<b>Varianza Explicada por los Componentes Principales</b>', template='plotly_dark')

fig.update_traces(line=dict(color='cyan'), marker=dict(color='white'))
fig.update_layout(title_x=0.5)

fig.show()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b><i>Insights</i></b></h3>
<h4>
Para cumplir con la funci√≥n de reducir dimensiones y evitar la multicolinealidad, se elige trabajar con 2 componentes principales, ya que estos explican el 99% de la varianza acumulada y permiten deshacerse de una cantidad importante de variables.</h4>
"""

# Retener suficientes componentes para capturar el 95% de la varianza
n_components = 2
pca = PCA(n_components=n_components)
X_pca_final = pca.fit_transform(X_scaled)

# Crear un DataFrame con los componentes principales
df_pca = pd.DataFrame(data=X_pca_final, columns=[f'PC{i+1}' for i in range(n_components)])
df_pca['PM2.5'] = df['PM2.5']
df_pca['Date'] = df['Date']
df_pca

pca.explained_variance_ratio_

fig = make_subplots(rows = 1, cols = len(df_pca.drop(['Date'], axis=1).columns))

for i, col in enumerate(df_pca.drop(['Date'], axis=1).columns):
  fig.add_trace(go.Box(y = df_pca[col], name = col), row=1, col = i+1)

fig.update_layout(height=600, width=1200, title_x = 0.5, title_text="<b>Boxplots de los Componentes Principales y PM 2.5</b>", template='plotly_dark')
fig.show()

fig = px.scatter(df_pca, x='PC1', y='PC2', color='PM2.5', title='<b>Visualizaci√≥n de part√≠culas PM 2.5 con 3/5 PCAs</b>',
                    template='plotly_dark', color_continuous_scale='haline_r', width=600)
fig.update_layout(title_x=0.5)
fig.show()

# Inspecci√≥n visual linealidad
fig = make_subplots(rows=1, cols=3, subplot_titles=df_pca.drop(['Date'], axis=1).columns)

for i, col in enumerate(df_pca.drop(['Date'], axis=1).columns):
    row = 1
    col_plot = (i % 3) + 1

    X = sm.add_constant(df_pca[col])
    model = sm.OLS(df_pca['PM2.5'], X).fit()
    predictions = model.predict(X)

    fig.add_trace(go.Scatter(x=df_pca[col], y=df_pca['PM2.5'], mode='markers', name=col), row=row, col=col_plot)
    fig.add_trace(go.Scatter(x=df_pca[col], y=predictions, mode='lines', name=col), row=row, col=col_plot)

fig.update_layout(height=500, width=1200, title_text="<b>Componentes Principales vs PM2.5</b>", template='plotly_dark', title_x=0.5)
fig.show()

corr = df_pca.drop(['Date'], axis=1).corr()

# Matriz sin repeticiones (tri√°ngulo inferior)
df_lt = corr.where(np.tril(np.ones(corr.shape)).astype(bool))

# Convertir la matriz a formato de texto, redondear y reemplazar ceros con cadenas vac√≠as
df_lt_text = df_lt.applymap(lambda x: f'{x:.4f}' if pd.notna(x) and x != 0 else '')

# Crear la gr√°fica de calor
fig = px.imshow(df_lt, text_auto=False, aspect="auto", color_continuous_scale='haline_r', zmin=-1, zmax=1,
                template='plotly_dark', height=600, width=1100)

# A√±adir un trazo alrededor de cada celda
fig.update_traces(hovertemplate=None, hoverinfo='skip', selector=dict(type='heatmap'), xgap=1, ygap=1, colorbar=dict(title='Value'))

# A√±adir texto personalizado a las celdas
fig.update_traces(text=df_lt_text.values, texttemplate='%{text}', textfont=dict(size=12))

# Configurar el t√≠tulo y las etiquetas
fig.update_layout(title='<b>Matriz de Correlaci√≥n de las 5 PCAs para la Regresi√≥n</b>', title_x=0.5, xaxis_title='PCA / PM 2.5', yaxis_title='PCA / PM 2.5')

# Mostrar la gr√°fica
fig.show()

# Pruebas de hip√≥tesis de los coeficientes de correlaci√≥n con PM 2.5
for col in df_pca.drop(['Date'], axis=1).corr().columns:
  corr_coef, p_value = pearsonr(df_pca['PM2.5'], df_pca[col])
  print(f"Correlaci√≥n entre PM2.5 y {col}:")
  print(f"Coeficiente: {corr_coef}")
  print(f"P-valor: {p_value}")
  if p_value < 0.05:
    print("La correlaci√≥n es estad√≠sticamente significativa. ‚úîÔ∏è‚Äã‚Äã\n")
  else:
    print("La correlaci√≥n NO es estad√≠sticamente significativa. ‚Äã‚ùå‚Äã‚Äã\n")

# Separamos las variables independientes y
X = df_pca.drop(['Date', 'PM2.5'], axis=1)
y = df_pca['PM2.5']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)

model = LinearRegression().fit(X_train, y_train)

y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

r2_train = r2_score(y_train, y_train_pred)
r2_test = r2_score(y_test, y_test_pred)
r2_adj_train = 1 - (1 - r2_train) * (len(y_train) - 1) / (len(y_train) - X_train.shape[1] - 1)
r2_adj_test = 1 - (1 - r2_test) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)

print(f"R^2 ajustado para el entrenamiento (con PCA): {r2_adj_train:.4f}")
print(f"R^2 ajustado para la prueba (con PCA): {r2_adj_test:.4f}")

fig = go.Figure()
fig.add_trace(go.Scatter(x = df_pca.index, y=df_pca['PM2.5'], mode='lines', name='PM2.5', line=dict(color='white')))
fig.add_trace(go.Scatter(x = X_train.index, y=y_train_pred, mode='lines', name='Predicci√≥n PM2.5 (Train)', line=dict(color='red')))
fig.add_trace(go.Scatter(x = X_test.index, y=y_test_pred, mode='lines', name='Predicci√≥n PM2.5 (Test)', line=dict(color='blue')))

fig.update_layout(title='<b>Regresi√≥n Lineal M√∫ltiple para PM2.5 con An√°lisis de Componentes Principales</b>', xaxis_title='Fecha', yaxis_title='PM2.5',
                  template='plotly_dark', title_x=0.5, legend_title_text='Valores')
fig.show()

y_pred_combined = np.concatenate([y_train_pred, y_test_pred])
y_original = np.concatenate([y_train, y_test])

n = len(y_pred_combined)

k = X_test.shape[1]  # Variables independientes
u = k
v = n - (k + 1)

# Predicciones y residuos
residuals = y_original - y_pred_combined

# Suma de cuadrados
SSR = np.sum((y_pred_combined - np.mean(y_original))**2)
SSE = np.sum(residuals**2)
SST = SSR + SSE

# Errores promedio
MSR = SSR / k
MSE = SSE / v

# F-Stat
F_cal = MSR / MSE

a = 0.05

# Valor cr√≠tico de F
F_critical = ss.f.ppf(1 - a, u, v)

# p-valor de F-Stat
p_value_F = 1 - ss.f.cdf(F_cal, u, v)

# Prueba Global del Modelo de Regresi√≥n Lineal M√∫ltiple para la Imputaci√≥n de NaNs en PM2.5
print(f"R^2 del modelo de regresi√≥n lineal m√∫ltiple: {r2_score(y_original, y_pred_combined):.4f}")

print(f"Estad√≠stico F asociado al modelo: {F_cal:.4f}")

print(f"Valor cr√≠tico de F (significancia = 0.05): {F_critical:.4f}\n")

print(f"p-value de F (Entrenamiento): {p_value_F:.4f}")

# Resultado de Prueba de hip√≥tesis
if p_value_F < a:
  print('Se rechaza H0. El modelo es estad√≠sticamente significativo.')
else:
  print('No se rechaza H0. El modelo es estad√≠sticamente significativo.')

# Definimos el n√∫mero de divisiones
tscv = TimeSeriesSplit(n_splits=5)

scores = cross_val_score(model, X, y, cv=tscv, scoring='r2')
scores_adj = 1 - (1 - scores) * (len(y) - 1) / (len(y) - X.shape[1] - 1)
print("Valores de R^2 ajustado de la validaci√≥n cruzada:", scores_adj)
print("Promedio de los valores de R^2 ajsutado:", scores_adj.mean())

# Calcular la varianza de los residuos
residual_sum_of_squares = np.sum(residuals**2)
degrees_of_freedom = len(y) - len(model.coef_) - 1
residual_variance = residual_sum_of_squares / degrees_of_freedom

# Calcular la varianza de los coeficientes
X_with_const = np.c_[np.ones(X.shape[0]), X]  # Agregar una columna de unos para el intercepto
cov_matrix = np.linalg.inv(X_with_const.T @ X_with_const) * residual_variance
standard_errors = np.sqrt(np.diag(cov_matrix))

# Calcular los valores t y los valores p
t_values = model.coef_ / standard_errors[1:]  # Excluir el error est√°ndar del intercepto
p_values = [2 * (1 - ss.t.cdf(np.abs(t), degrees_of_freedom)) for t in t_values]

coef_df = pd.DataFrame({'Variable': X.columns, 'Coeficiente': model.coef_,
                        'Error Est√°ndar': standard_errors[1:], 'Valor t': t_values, 'Valor p': p_values})

intercept_df = pd.DataFrame({'Variable': ['Intersecci√≥n'], 'Coeficiente': [model.intercept_],
                             'Error Est√°ndar': [standard_errors[0]], 'Valor t': [model.intercept_ / standard_errors[0]],
                             'Valor p': [2 * (1 - ss.t.cdf(np.abs(model.intercept_ / standard_errors[0]), degrees_of_freedom))]})

summary_df = pd.concat([coef_df, intercept_df], ignore_index=True)
summary_df['Significativo'] = summary_df['Valor p'] < 0.05
summary_df

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Resultados</b></h3>
<h4>
Aunque la regresi√≥n lineal m√∫ltiple sin PCA tambi√©n muestra un R<sup>2</sup> ajustado bajo (0.2483), la p√©rdida de informaci√≥n al aplicar PCA es notable. Esto se refleja en la menor capacidad explicativa del modelo con PCA. A pesar de que ambos modelos son estad√≠sticamente significativos seg√∫n el p-valor del estad√≠stico F, el modelo sin PCA mantiene la estructura original de los datos, preservando m√°s informaci√≥n, lo que resulta en un desempe√±o relativamente mejor.</h4>

<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Implementaci√≥n del An√°lisis de Conglomerados con <i>K-Means</i></b></h3>
<h4><b>Modelo Con An√°lisis de Componentes Principales (PCA)</b></h4>
"""

# Seleccionamos columnas num√©ricas
X = df_pca.drop(columns=['Date'])

# Estandarizaci√≥n del PCA
X_scaled = StandardScaler().fit_transform(X)

# Rango de K
range_n_clusters = range(2, 11)

# Coeficientes de silueta
silhouette_scores = []
for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Se organizan los resultados del coeficiente
silhouette_df = pd.DataFrame({'n_clusters': range_n_clusters, 'silhouette_score': silhouette_scores})

# Se grafican los coeficientes
fig = px.line(silhouette_df, x='n_clusters', y='silhouette_score',
              title='<b>Coeficiente de la Silueta para Distintos Clusters</b>',
              labels={'n_clusters': 'Number of Clusters', 'silhouette_score': 'Silhouette Score'}, template='plotly_dark', color_discrete_sequence=['blue'])
fig.update_layout(title_x=0.5)
fig.show()

# Apply KMeans with k = 3
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(X_scaled)

# Add cluster labels to the DataFrame
df_pca['cluster'] = cluster_labels

# Graficamos los datos
fig = px.scatter_3d(df_pca, x='PC1', y='PC2', z='PM2.5',
                    color='cluster', title='<b>Agrupamiento para K = 3 Con PCA</b>', template='plotly_dark', width=800)
fig.update_layout(title_x=0.5)
fig.show()

# Inspecci√≥n visual linealidad
fig = make_subplots(rows=1, cols=2, subplot_titles=df_pca.drop(['Date'], axis=1).columns)

# Colores por cl√∫ster
cluster_colors = {0: 'blue', 1: 'yellow', 2: 'red'}

for i, col in enumerate(df_pca.drop(['Date'], axis=1).columns):
  row = 1
  col_plot = (i % 2) + 1

  fig.add_trace(go.Scatter(x=df_pca[col], y=df_pca['PM2.5'], mode='markers', marker_color=df_pca['cluster'], showlegend=False), row=row, col=col_plot)

fig.update_layout(height=500, width=1100, title_text="<b>Predictores vs PM2.5 por Cluster con PCA</b>", title_x=0.5, template='plotly_dark')
fig.show()

num_cols = list(df_pca.select_dtypes(include=numerics).columns)
df_pca.groupby('cluster')[num_cols].mean()

"""<font face = "Times New Roman" size = "3" color = "black">
<div align = "justify">
<h3><b>Resultados</b></h3>
<h4>El an√°lisis de <i>K-means</i> utilizando PCA con K = 3, es decir, 3 conglomerados, result√≥ en un coeficiente de silueta de 0.3687, lo que indica una mejor separaci√≥n y cohesi√≥n de los grupos en comparaci√≥n con el modelo sin PCA, que obtuvo un coeficiente de 0.29. Esto sugiere que la reducci√≥n de dimensionalidad mediante PCA ayud√≥ a identificar clusters m√°s definidos, mejorando la calidad de la agrupaci√≥n.</h4>

<h2><b>Conclusiones</b></h2>
<h3>El an√°lisis de la contaminaci√≥n del aire en el √Årea Metropolitana de Monterrey mediante t√©cnicas estad√≠sticas avanzadas y de aprendizaje autom√°tico revela importantes insights sobre los niveles de PM2.5 y sus factores asociados. La regresi√≥n lineal m√∫ltiple con PCA mostr√≥ problemas de sobreajuste, sugiriendo que el PCA puede no ser la mejor estrategia para este conjunto de datos debido a la p√©rdida de informaci√≥n. Sin embargo, el an√°lisis de <i>K-Means</i> con K = 3 proporcion√≥ una mejor agrupaci√≥n de los datos en comparaci√≥n con K = 2. Estos hallazgos destacan la necesidad de explorar t√©cnicas de modelado adicionales y de ajustar los m√©todos anal√≠ticos para mejorar la precisi√≥n de las predicciones y la comprensi√≥n de los patrones de contaminaci√≥n en la regi√≥n.
Para mejorar los resultados, se podr√≠an explorar otros enfoques como la implementaci√≥n de modelos de machine learning m√°s robustos, como Random Forest o Gradient Boosting, que pueden manejar de mejor manera las relaciones no lineales entre las variables.
</h3>

<h2><b>Referencias</b></h2>
<h3>
<ol>
<li>Aggarwal, S. (2020, mayo 13). Multiple linear Regression. Towards Data Science. Recuperado de <a href = "https://towardsdatascience.com/multiple-linear-regression-8cf3bee21d8b"><i>Multiple Linear Regression</i></a></li>
<li>Aldean, A. S. (2023, junio 29). Time series data interpolation. Medium. Recuperado de <a href = "https://medium.com/@aseafaldean/time-series-data-interpolation-e4296664b86"><i>Time series: data interpolation</i></a></li>
<li>Chatterjee, S. & S. Hadi, A. (2012). <i>Regression Analysis by Example. John Wiley and Sons (WIE)</i>. Recuperado de <a href = "https://sadbhavnapublications.org/research-enrichment-material/2-Statistical-Books/Regression-Analysis-by-Example.pdf"><i>Regression Analysis by Example</i></a></li>
<li>Conceptos b√°sicos sobre el material particulado (PM, por sus siglas en ingl√©s). (2024, junio 25). Agencia de Protecci√≥n Ambiental de Estados Unidos (EPA). Recuperado de <a href = "https://espanol.epa.gov/espanol/conceptos-basicos-sobre-el-material-particulado-pm-por-sus-siglas-en-ingles">Conceptos b√°sicos sobre part√≠culas PM</a></li>
<li>Contaminaci√≥n del aire: causas y tipos. (2019, julio 30). Fundaci√≥n Aquae. Recuperado de <a href = "https://www.fundacionaquae.org/wiki/causas-y-tipos-de-la-contaminacion-del-aire/">Causas y Tipos de la Contaminaci√≥n del Aire</a></li>
<li><i>Data Science Wizards</i>. (2023, noviembre 3). <i>Preprocessing and data exploration for time series ‚Äî handling missing values. Medium</i>. Recuperado de <a href = "https://medium.com/@datasciencewizards/preprocessing-and-data-exploration-for-time-series-handling-missing-values-e5c507f6c71c"><i>Preprocessing and data exploration for time series</i></a></li>
<li><i>Fill the gap: EDDI for multivariate time series missing value imputation</i>. (s/f). Techcommunity.microsoft.com. Recuperado el 1 de septiembre de 2024, de <a href = "https://techcommunity.microsoft.com/t5/microsoft-developer-community/fill-the-gap-eddi-for-multivariate-time-series-missing-value/ba-p/3289782"><i>EDDI for multivariate time series missing value imputation</i></a></li>
<li>Gupta, S. (2022, febrero 8). <i>Pre-processing of time series data. EnjoyAlgorithms </i>. Recuperado de <a href = "https://medium.com/enjoy-algorithm/pre-processing-of-time-series-data-c50f8a3e7a98"><i>Pre-processing of time series data</i></a></li>
<li>Hartigan, J. A., & Wong, M. A. (1979). <i>Algorithm AS 136: A K-means clustering algorithm. Journal of the Royal Statistical Society. Series C, Applied statistics </i>, 28(1), 100. Recuperado de <a href = "https://doi.org/10.2307/2346830"><i>A K-means clustering algorithm</i></a></li>
<li>Hern√°ndez, F., Usuga, O., & Mazo, M. (2024). An√°lisis de Regresi√≥n con R. Recuperado de <a href = "https://fhernanb.github.io/libro_regresion/">An√°lisis de Regresi√≥n con R</a></li>
<li>Hyndman, R. J., & Athanasopoulos, G. (2018). <i>Forecasting: Principles and Practice. (2nd ed.) OTexts</i>. Recuperado de <a href = "https://otexts.org/fpp2/"><i>Forecasting: Principles and Practice</i></a></li>
<li><i>Imputing time series missing values. (2020, diciembre 18). Crawstat </i>. Recuperado de <a href = "https://crawstat.com/2020/12/18/imputing-time-series-missing-values/"><i>Imputing time series missing values</i></a></li>
<li><i>LinearRegression. (s/f). Scikit-Learn</i>. Recuperado el 1 de septiembre de 2024, de <a href = "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"><i>Sklearn: Linear Regression</i></a></li>
<li>Mart√≠nez, M. P. Z. (2023, enero 3). Calidad del aire en Zona Metropolitana: entre regular y mala. Reporte √çndigo. Recuperado de <a href = "https://www.reporteindigo.com/reporte/calidad-del-aire-en-zona-metropolitana-entre-regular-y-mala/">Calidad del aire en Zona Metropolitana de MTY</a></li>
<li><i>pandas.DataFrame.bfill ‚Äî pandas 2.2.2 documentation. (s/f). Pydata.org</i>. Recuperado el 1 de septiembre de 2024, de <a href = "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.bfill.html"><i>Pandas: BFill</i></a></li>
<li><i>pandas.DataFrame.ffill ‚Äî pandas 2.2.2 documentation. (s/f). Pydata.org</i>. Recuperado el 1 de septiembre de 2024, de <a href = "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ffill.html"><i>Pandas: FFill</i></a></li>
<li><i>pandas.dataframe.interpolate ‚Äî pandas 2.2.2 documentation. (s/f). Pydata.org</i>. Recuperado el 1 de septiembre de 2024, de <a href = "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html"><i>Pandas: Interpolate</i></a></li>
<li><i>Python</i>. (2019, mayo 23). <i>GeeksforGeeks</i>. Recuperado de <a href = "https://www.geeksforgeeks.org/python-linear-regression-using-sklearn/"><i>Python: Linear Regression using Sklearn</i></a></li>
<li>Sanz, F. (2020, noviembre 26). Algoritmo K-Means Clustering ‚Äì aplicaciones y desventajas. The Machine Learners. Recuperado de <a href = "https://www.themachinelearners.com/k-means/">Algoritmo K-Means Clustering</a></li>
<li>Segu√≠, P. (2020, marzo 12). 9 Formas inusuales que la contaminaci√≥n del aire afecta tu salud. OVACEN. Recuperado de <a href = "https://ovacen.com/contaminacion-aire/">9 Formas inusuales que la contaminaci√≥n del aire afecta tu salud</a></li>
<li>Snyder, et al. (2017). <i>Contribution of systemic and somatic factors to clinical response and resistance to PD-L1 blockade in urothelial cancer: An exploratory multi-omic analysis. PLoS Medicine</i>, 14(5), e1002309. Recuperado de <a href = "https://doi.org/10.1371/journal.pmed.1002309">Contribution of systemic and somatic factors to clinical response and resistance to PD-L1 blockade in urothelial cancer: An exploratory multi-omic analysis. PLoS Medicine</a></li></li>
<li>Taylor, J. W. (2003). <i>Exponential smoothing with a damped multiplicative trend. International Journal of Forecasting</i>, 19(4), 715‚Äì725. Recuperado de <a href = "https://doi.org/10.1016/s0169-2070(03)00003-7">Exponential smoothing with a damped multiplicative trend. International Journal of Forecasting</a></li>
</ol>
</h3>
"""

